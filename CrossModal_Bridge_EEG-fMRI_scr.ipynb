{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG-fMRI Bridge Fusion Pipeline\n",
    "\n",
    "Cross-modal fusion of EEG tri-modal features (ERP, Power Spectrum, Connectivity) and fMRI features\n",
    "(activation + connectivity) via a learned bridge network with cross-modal attention.\n",
    "\n",
    "**Pipeline overview**\n",
    "1. Load EEG & fMRI raw data and clinical labels\n",
    "2. Build aligned `BridgeRawDataset`\n",
    "3. Load pre-trained EEG & fMRI encoders and extract frozen features\n",
    "4. Train `EEGfMRIBridgeFusionNet` with Leave-One-Out Cross-Validation (LOOCV)\n",
    "5. Explainability analyses (Gradient Saliency, Integrated Gradients, SHAP, Attention/Fusion weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Configuration\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import copy\n",
    "import h5py\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import logging\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "matplotlib.use('Agg')\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "from scipy.io import loadmat\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,\n",
    "                              roc_auc_score, confusion_matrix, roc_curve)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add pipeline directories to path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'EEG_CODE'))\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'fMRI_CODE'))\n",
    "\n",
    "# Import shared EEG components\n",
    "from EEG_CODE.crossmodal_v4_enhancements import (\n",
    "    EnhancedTriModalFusionNetV4,\n",
    "    LearnedFusionModule,\n",
    "    EnhancedERPEncoder,\n",
    "    EnhancedPowerEncoder,\n",
    "    get_fusion_weights_from_model\n",
    ")\n",
    "\n",
    "# Import fMRI components (avoid redefinition)\n",
    "from fMRI_CODE.fmri_utils import fMRIFusionNet, ActivationEncoder, ConnectivityEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BridgeConfig:\n",
    "    def __init__(self):\n",
    "        self.eeg_checkpoint_dir = Path('./EEG_CODE/checkpoints')\n",
    "        self.fmri_checkpoint_dir = Path('./fMRI_CODE/checkpoints_fmri')\n",
    "        self.eeg_hidden_dim = 128\n",
    "        self.fmri_hidden_dim = 64\n",
    "        self.bridge_hidden_dim = 128\n",
    "        self.num_classes = 2\n",
    "        self.overlap_subjects = list(range(1, 33))\n",
    "        self.batch_size = 8\n",
    "        self.num_epochs = 50\n",
    "        self.lr = 1e-4\n",
    "        self.weight_decay = 1e-4\n",
    "        self.patience = 10\n",
    "        self.grad_clip = 1.0\n",
    "        self.dropout = 0.3\n",
    "        self.eeg_base_path = Path(os.getenv('EEG_DATA_PATH', r'E:\\Head_neck'))\n",
    "        self.eeg_path_pw = self.eeg_base_path / 'EEG' / 'DATA' / 'PROC' / 'data_proc' / 'cleaned_data' / 'TF_dir' / 'pwspctrm' / 'PWS' / 'feat'\n",
    "        self.eeg_path_erp = self.eeg_base_path / 'EEG' / 'DATA' / 'PROC' / 'data_proc' / 'cleaned_data' / 'TF_dir' / 'ERP' / 'New'\n",
    "        self.eeg_path_conn = self.eeg_base_path / 'EEG' / 'DATA' / 'PROC' / 'data_proc' / 'cleaned_data' / 'conn_dir' / 'CONN' / 'New'\n",
    "        self.eeg_label_path = self.eeg_base_path / 'EEG' / 'DATA' / 'PROC' / 'data_proc' / 'cleaned_data' / 'TF_dir'\n",
    "        self.fmri_base_path = Path(r'E:\\Head_neck\\fMRI')\n",
    "        self.fmri_data_dir = self.fmri_base_path\n",
    "        self.fmri_activation_types = ['sensory', 'AN', 'LN', 'cognitive', 'DMN']\n",
    "        self.fmri_connectivity_types = ['DMN']\n",
    "        self.fmri_agg_method = 'both'\n",
    "        self.bands = {'alpha': 'Alpha', 'beta': 'Beta', 'theta': 'Theta'}\n",
    "        self.eeg_segments = ['1_Hz', '2_Hz', '4_Hz', '6_Hz', '8_Hz', '10_Hz', '12_Hz',\n",
    "                             '14_Hz', '16_Hz', '18_Hz', '20_Hz', '25_Hz', '30_Hz', '40_Hz']\n",
    "        self.func_segments = ['open', 'close']\n",
    "        self.output_dir = Path('./results_bridge')\n",
    "        self.checkpoint_dir = Path('./checkpoints_bridge')\n",
    "        self.log_dir = Path('./logs_bridge')\n",
    "        for d in [self.output_dir, self.checkpoint_dir, self.log_dir]:\n",
    "            d.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reproducibility & Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "config = BridgeConfig()\n",
    "log_file = config.log_dir / 'bridge_fusion.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler(log_file), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger('bridge_fusion')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "logger.info(f'Device: {device}')\n",
    "logger.info(f'Bridge Config: overlap_subjects={len(config.overlap_subjects)}, '\n",
    "            f'bridge_hidden_dim={config.bridge_hidden_dim}')\n",
    "print('Cell complete: Imports & configuration loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EEG Model Wrapper\n",
    "\n",
    "The fMRI model classes (`fMRIFusionNet`, `ActivationEncoder`, `ConnectivityEncoder`) are\n",
    "imported from `fMRI_CODE.fmri_utils` to avoid redefinition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEG Model Wrapper\n",
    "class ImprovedTriModalFusionNet(nn.Module):\n",
    "    def __init__(self, in_pw_dim, in_erp_dim, in_conn_dim,\n",
    "                 fusion_dim=128, num_classes=2, dropout=0.3,\n",
    "                 num_transformer_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.model = EnhancedTriModalFusionNetV4(\n",
    "            erp_channels=in_erp_dim,\n",
    "            pw_channels=in_pw_dim,\n",
    "            conn_features=in_conn_dim,\n",
    "            hidden_dim=fusion_dim,\n",
    "            num_classes=num_classes,\n",
    "            dropout=dropout,\n",
    "            num_transformer_layers=num_transformer_layers,\n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        self.fusion_weight_history = []\n",
    "\n",
    "    def forward(self, erp, pw, conn, return_feats=False):\n",
    "        if return_feats:\n",
    "            logits, fusion_weights, fused_feats = self.model(\n",
    "                erp, pw, conn,\n",
    "                return_fusion_weights=True,\n",
    "                return_fused_feats=True\n",
    "            )\n",
    "            return {\n",
    "                'logits': logits,\n",
    "                'gates': fusion_weights,\n",
    "                'fused_feats': fused_feats\n",
    "            }\n",
    "        else:\n",
    "            return self.model(erp, pw, conn)\n",
    "\n",
    "    def get_fusion_weights(self):\n",
    "        return get_fusion_weights_from_model(self.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading \u00e2\u20ac\u201d EEG Side\n",
    "\n",
    "Uses wildcard glob patterns (matching `run_training_lite.py`) with both 3-digit and 2-digit\n",
    "subject ID fallback to ensure PW and CONN files are found regardless of naming convention.\n",
    "Labels come from `medical_score.csv` only \u00e2\u20ac\u201d this is the single source of truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading - EEG Side (robust wildcard globs)\n",
    "\n",
    "def load_eeg_conn_features(conn_dir, subject_list, band_list, cond_list):\n",
    "    \"\"\"Load EEG connectivity features from .mat files using wildcard patterns.\"\"\"\n",
    "    conn_features = {}\n",
    "    for subj in subject_list:\n",
    "        for subj_str in [f'{subj:03d}', f'{subj:02d}']:\n",
    "            for band_key, band_name in band_list.items():\n",
    "                for cond in cond_list:\n",
    "                    # Wildcard pattern: match any file containing sub+id, band, cond\n",
    "                    pattern = str(conn_dir / f'*sub{subj_str}*{band_key}*{cond}*.mat')\n",
    "                    files = sorted(glob.glob(pattern))\n",
    "                    if not files:\n",
    "                        # Try with capitalized band name\n",
    "                        pattern = str(conn_dir / f'*sub{subj_str}*{band_name}*{cond}*.mat')\n",
    "                        files = sorted(glob.glob(pattern))\n",
    "                    if not files:\n",
    "                        # Try exact legacy pattern\n",
    "                        pattern = str(conn_dir / f'conn_{band_name}_{cond}_sub{subj_str}.mat')\n",
    "                        files = sorted(glob.glob(pattern))\n",
    "                    for f in files:\n",
    "                        try:\n",
    "                            mat = loadmat(f)\n",
    "                            for k in mat:\n",
    "                                if not k.startswith('_'):\n",
    "                                    data = np.array(mat[k], dtype=np.float32).flatten()\n",
    "                                    data = np.nan_to_num(data, nan=0.0)\n",
    "                                    conn_key = (subj, band_key, cond, 0)\n",
    "                                    conn_features[conn_key] = data\n",
    "                                    break\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f'Error loading {f}: {e}')\n",
    "    logger.info(f'Loaded {len(conn_features)} EEG connectivity samples')\n",
    "    return conn_features\n",
    "\n",
    "\n",
    "def load_eeg_pw_features(pw_dir, subject_list, band_list, freq_list):\n",
    "    \"\"\"Load EEG power spectrum features from .mat files using wildcard patterns.\"\"\"\n",
    "    pw_features = {}\n",
    "    for subj in subject_list:\n",
    "        for subj_str in [f'{subj:03d}', f'{subj:02d}']:\n",
    "            for band in band_list:\n",
    "                for freq in freq_list:\n",
    "                    pattern = str(pw_dir / f'*sub{subj_str}*{band}*{freq}*.mat')\n",
    "                    files = sorted(glob.glob(pattern))\n",
    "                    if not files:\n",
    "                        # Legacy exact pattern\n",
    "                        pattern = str(pw_dir / f'powspctrm_{band}_{freq}_sub{subj_str}.mat')\n",
    "                        files = sorted(glob.glob(pattern))\n",
    "                    for f in files:\n",
    "                        try:\n",
    "                            mat = loadmat(f)\n",
    "                            for k in mat:\n",
    "                                if not k.startswith('_'):\n",
    "                                    data = np.array(mat[k], dtype=np.float32).flatten()\n",
    "                                    data = np.nan_to_num(data, nan=0.0)\n",
    "                                    pw_key = (subj, band, freq, 0)\n",
    "                                    pw_features[pw_key] = data\n",
    "                                    break\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f'Error loading {f}: {e}')\n",
    "    logger.info(f'Loaded {len(pw_features)} EEG power spectrum samples')\n",
    "    return pw_features\n",
    "\n",
    "\n",
    "def load_eeg_erp_features(erp_dir, subject_list, band_list, freq_list):\n",
    "    \"\"\"Load EEG ERP features from .mat/.h5 files using wildcard patterns.\"\"\"\n",
    "    erp_features = {}\n",
    "    for subj in subject_list:\n",
    "        for subj_str in [f'{subj:03d}', f'{subj:02d}']:\n",
    "            for band in band_list:\n",
    "                for freq in freq_list:\n",
    "                    pattern = str(erp_dir / f'*sub{subj_str}*{band}*{freq}*.mat')\n",
    "                    erp_files = sorted(glob.glob(pattern))\n",
    "                    if not erp_files:\n",
    "                        pattern = str(erp_dir / f'ERP_sub{subj_str}_{band}_{freq}*.mat')\n",
    "                        erp_files = sorted(glob.glob(pattern))\n",
    "                    for f in erp_files:\n",
    "                        try:\n",
    "                            with h5py.File(f, 'r') as hf:\n",
    "                                if 'erp_struct' in hf:\n",
    "                                    erp_group = hf['erp_struct']\n",
    "                                elif 'erp' in hf:\n",
    "                                    erp_group = hf['erp']\n",
    "                                else:\n",
    "                                    erp_group = hf[list(hf.keys())[0]]\n",
    "\n",
    "                                if 'avg' in erp_group:\n",
    "                                    data = np.array(erp_group['avg'], dtype=np.float32)\n",
    "                                elif 'trial' in erp_group:\n",
    "                                    data = np.array(erp_group['trial'], dtype=np.float32)\n",
    "                                    if data.ndim == 3:\n",
    "                                        data = np.mean(data, axis=0)\n",
    "                                else:\n",
    "                                    for dk in erp_group.keys():\n",
    "                                        candidate = erp_group[dk]\n",
    "                                        if hasattr(candidate, 'shape') and len(candidate.shape) >= 2:\n",
    "                                            data = np.array(candidate, dtype=np.float32)\n",
    "                                            break\n",
    "                                    else:\n",
    "                                        continue\n",
    "\n",
    "                                data = np.nan_to_num(data, nan=0.0)\n",
    "                                erp_key = (subj, band, freq, 0)\n",
    "                                erp_features[erp_key] = data\n",
    "                        except Exception:\n",
    "                            try:\n",
    "                                mat = loadmat(f)\n",
    "                                for k in mat:\n",
    "                                    if not k.startswith('_'):\n",
    "                                        data = np.array(mat[k], dtype=np.float32)\n",
    "                                        data = np.nan_to_num(data, nan=0.0)\n",
    "                                        erp_key = (subj, band, freq, 0)\n",
    "                                        erp_features[erp_key] = data\n",
    "                                        break\n",
    "                            except Exception as e2:\n",
    "                                logger.warning(f'Error loading ERP {f}: {e2}')\n",
    "    logger.info(f'Loaded {len(erp_features)} EEG ERP samples')\n",
    "    return erp_features\n",
    "\n",
    "\n",
    "def load_eeg_labels(label_dir, binary=True):\n",
    "    \"\"\"Load EEG clinical labels from medical_score.csv (single source of truth).\"\"\"\n",
    "    csv_path = os.path.join(label_dir, 'medical_score.csv')\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f'Label file not found: {csv_path}')\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=['Postoperative evaluation'])\n",
    "    if df['Subject'].dtype == object:\n",
    "        df['subject_id'] = df['Subject'].str.replace('sub', '', regex=False).astype(int)\n",
    "    else:\n",
    "        df['subject_id'] = df['Subject'].astype(int)\n",
    "    label_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        subj = int(row['subject_id'])\n",
    "        score = row['Postoperative evaluation']\n",
    "        label_dict[subj] = 0 if score <= 2 else 1 if binary else score\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "# Load EEG data (filtered to subjects 1-32)\n",
    "logger.info('Loading EEG data for subjects 1-32...')\n",
    "eeg_label_dict = load_eeg_labels(str(config.eeg_label_path))\n",
    "logger.info(f'EEG labels: {len(eeg_label_dict)} subjects')\n",
    "band_keys = list(config.bands.keys())\n",
    "eeg_erp_features = load_eeg_erp_features(\n",
    "    config.eeg_path_erp, config.overlap_subjects, band_keys, config.eeg_segments)\n",
    "eeg_pw_features = load_eeg_pw_features(\n",
    "    config.eeg_path_pw, config.overlap_subjects, band_keys, config.eeg_segments)\n",
    "eeg_conn_features = load_eeg_conn_features(\n",
    "    config.eeg_path_conn, config.overlap_subjects, config.bands, config.func_segments)\n",
    "\n",
    "logger.info(f'EEG data loaded: ERP={len(eeg_erp_features)}, PW={len(eeg_pw_features)}, CONN={len(eeg_conn_features)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Loading \u00e2\u20ac\u201d fMRI Side\n",
    "\n",
    "fMRI activation and connectivity features are loaded per subject.\n",
    "Labels use `eeg_label_dict` (from `medical_score.csv`) as the single source of truth \u00e2\u20ac\u201d\n",
    "both modalities predict the same postoperative outcome, so no separate fMRI label file is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading - fMRI Side\n",
    "def load_fmri_activation_features(data_dir, subject_list, activation_types, agg_method='both'):\n",
    "    features = {}\n",
    "    for subj in tqdm(subject_list, desc='Loading fMRI activations'):\n",
    "        subj_features = []\n",
    "        subj_dir = data_dir / f'sub-{subj}'\n",
    "        for act_type in activation_types:\n",
    "            filepath = subj_dir / f'subject_{subj}_activation_{act_type}.csv'\n",
    "            if not filepath.exists():\n",
    "                continue\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                if 'Subject' in df.columns:\n",
    "                    df = df.drop('Subject', axis=1)\n",
    "                data = df.values.astype(np.float32)\n",
    "                data = np.nan_to_num(data, nan=0.0)\n",
    "                if agg_method == 'mean':\n",
    "                    agg_data = np.mean(data, axis=0)\n",
    "                elif agg_method == 'std':\n",
    "                    agg_data = np.std(data, axis=0)\n",
    "                elif agg_method == 'both':\n",
    "                    agg_data = np.concatenate([np.mean(data, axis=0), np.std(data, axis=0)])\n",
    "                else:\n",
    "                    raise ValueError(f'Unknown agg method: {agg_method}')\n",
    "                subj_features.append(agg_data)\n",
    "            except Exception as e:\n",
    "                logger.warning(f'Error loading {filepath}: {e}')\n",
    "        if subj_features:\n",
    "            features[subj] = torch.tensor(np.concatenate(subj_features), dtype=torch.float32)\n",
    "    logger.info(f'fMRI activation features: {len(features)} subjects')\n",
    "    if features:\n",
    "        sample = list(features.values())[0]\n",
    "        logger.info(f'  Activation feature dim: {sample.shape[0]}')\n",
    "    return features\n",
    "\n",
    "def load_fmri_connectivity_features(data_dir, subject_list, connectivity_types):\n",
    "    \"\"\"Load fMRI connectivity features.\"\"\"\n",
    "    features = {}\n",
    "    for subj in tqdm(subject_list, desc='Loading fMRI connectivity'):\n",
    "        subj_features = []\n",
    "        subj_dir = data_dir / f'sub-{subj}'\n",
    "        for conn_type in connectivity_types:\n",
    "            filepath = subj_dir / f'subject_{subj}_fdr_PPI_Connectivity_{conn_type}.csv'\n",
    "            if not filepath.exists():\n",
    "                continue\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                if 'Subject' in df.columns:\n",
    "                    df = df.drop('Subject', axis=1)\n",
    "                data = df.values.astype(np.float32).flatten()\n",
    "                data = np.nan_to_num(data, nan=0.0)\n",
    "                subj_features.append(data)\n",
    "            except Exception as e:\n",
    "                logger.warning(f'Error loading {filepath}: {e}')\n",
    "        if subj_features:\n",
    "            features[subj] = torch.tensor(np.concatenate(subj_features), dtype=torch.float32)\n",
    "    logger.info(f'fMRI connectivity features: {len(features)} subjects')\n",
    "    if features:\n",
    "        sample = list(features.values())[0]\n",
    "        logger.info(f'  Connectivity feature dim: {sample.shape[0]}')\n",
    "    return features\n",
    "\n",
    "# Load fMRI data\n",
    "logger.info('Loading fMRI data...')\n",
    "fmri_act_features = load_fmri_activation_features(\n",
    "    config.fmri_data_dir, config.overlap_subjects,\n",
    "    config.fmri_activation_types, config.fmri_agg_method)\n",
    "fmri_conn_features = load_fmri_connectivity_features(\n",
    "    config.fmri_data_dir, config.overlap_subjects, config.fmri_connectivity_types)\n",
    "logger.info(f'fMRI data loaded: Act={len(fmri_act_features)}, Conn={len(fmri_conn_features)}')\n",
    "\n",
    "# Single source of truth for labels: eeg_label_dict (medical_score.csv)\n",
    "bridge_labels = {subj: eeg_label_dict[subj] for subj in config.overlap_subjects if subj in eeg_label_dict}\n",
    "logger.info(f'Bridge labels: {len(bridge_labels)} subjects')\n",
    "logger.info(f'Class distribution: {dict(zip(*np.unique(list(bridge_labels.values()), return_counts=True)))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Subject Alignment & Bridge Dataset\n",
    "\n",
    "`BridgeRawDataset` now gracefully degrades when PW or CONN data is missing for a subject:\n",
    "- If a subject has ERP but not PW/CONN, the missing modalities are zero-padded\n",
    "- `min()`/`max()` calls are guarded against empty lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject Alignment & Bridge Dataset (with graceful degradation)\n",
    "\n",
    "class BridgeRawDataset(Dataset):\n",
    "    def __init__(self, eeg_erp, eeg_pw, eeg_conn, fmri_act, fmri_conn,\n",
    "                 labels, subject_list, bands, func_segments):\n",
    "        self.samples = []\n",
    "\n",
    "        # Determine reference shapes from available data for zero-padding\n",
    "        pw_shapes = [v.shape for v in eeg_pw.values()]\n",
    "        conn_shapes = [v.shape for v in eeg_conn.values()]\n",
    "        ref_pw_shape = pw_shapes[0] if pw_shapes else None\n",
    "        ref_conn_shape = conn_shapes[0] if conn_shapes else None\n",
    "\n",
    "        # Build per-subject EEG sample lists\n",
    "        eeg_by_subj = defaultdict(list)\n",
    "        for key, erp_val in eeg_erp.items():\n",
    "            subj = int(key[0]) if not isinstance(key[0], int) else key[0]\n",
    "            pw_val = eeg_pw.get(key)\n",
    "            lookup_band = str(key[1]).lower()\n",
    "            conn_val = None\n",
    "            for cond in func_segments:\n",
    "                conn_key = (key[0], lookup_band, cond, key[3])\n",
    "                if conn_key in eeg_conn:\n",
    "                    conn_val = eeg_conn[conn_key]\n",
    "                    break\n",
    "\n",
    "            # Graceful degradation: zero-pad missing PW/CONN\n",
    "            if pw_val is None and ref_pw_shape is not None:\n",
    "                pw_val = np.zeros(ref_pw_shape, dtype=np.float32)\n",
    "                logger.debug(f'Subject {subj}: zero-padded missing PW for key {key}')\n",
    "            if conn_val is None and ref_conn_shape is not None:\n",
    "                conn_val = np.zeros(ref_conn_shape, dtype=np.float32)\n",
    "                logger.debug(f'Subject {subj}: zero-padded missing CONN for key {key}')\n",
    "\n",
    "            if pw_val is not None and conn_val is not None:\n",
    "                eeg_by_subj[subj].append((erp_val, pw_val, conn_val))\n",
    "\n",
    "        # Align subjects\n",
    "        for subj in sorted(subject_list):\n",
    "            s_id = int(subj)\n",
    "            has_eeg = s_id in eeg_by_subj\n",
    "            has_fmri_act = s_id in fmri_act\n",
    "            has_fmri_conn = s_id in fmri_conn\n",
    "            has_label = s_id in labels\n",
    "            if has_eeg and has_fmri_act and has_fmri_conn and has_label:\n",
    "                self.samples.append({\n",
    "                    'subject': s_id,\n",
    "                    'label': labels[s_id],\n",
    "                    'eeg_samples': eeg_by_subj[s_id],\n",
    "                    'fmri_act': fmri_act[s_id],\n",
    "                    'fmri_conn': fmri_conn[s_id],\n",
    "                })\n",
    "            else:\n",
    "                missing = []\n",
    "                if not has_eeg: missing.append(\"EEG\")\n",
    "                if not has_fmri_act: missing.append(\"fMRI-Act\")\n",
    "                if not has_fmri_conn: missing.append(\"fMRI-Conn\")\n",
    "                if not has_label: missing.append(\"Label\")\n",
    "                logger.debug(f\"Subject {s_id} excluded. Missing: {', '.join(missing)}\")\n",
    "\n",
    "        if len(self.samples) == 0:\n",
    "            logger.error(\"!!! NO ALIGNED SUBJECTS FOUND !!! Check Subject IDs and file paths.\")\n",
    "            return\n",
    "        logger.info(f'BridgeRawDataset: {len(self.samples)} aligned subjects')\n",
    "        eeg_counts = [len(s[\"eeg_samples\"]) for s in self.samples]\n",
    "        if eeg_counts:\n",
    "            logger.info(f'  EEG samples per subject: min={min(eeg_counts)}, max={max(eeg_counts)}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        return s['eeg_samples'], s['fmri_act'], s['fmri_conn'], s['label'], s['subject']\n",
    "\n",
    "\n",
    "bridge_raw_dataset = BridgeRawDataset(\n",
    "    eeg_erp_features, eeg_pw_features, eeg_conn_features,\n",
    "    fmri_act_features, fmri_conn_features,\n",
    "    bridge_labels, config.overlap_subjects,\n",
    "    config.bands, config.func_segments\n",
    ")\n",
    "print(f'\\nAligned subjects: {len(bridge_raw_dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== BRIDGE SUBJECT DATA HANDLER ==========\n",
    "class BridgeSubjectDataHandler:\n",
    "    \"\"\"Centralizes data management for the EEG-fMRI bridge pipeline.\n",
    "    Handles EEG feature loading, fMRI feature loading, label alignment,\n",
    "    subject intersection, and LOSO splitting.\"\"\"\n",
    "\n",
    "    def __init__(self, config, logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.eeg_erp_features = {}\n",
    "        self.eeg_pw_features = {}\n",
    "        self.eeg_conn_features = {}\n",
    "        self.fmri_act_features = {}\n",
    "        self.fmri_conn_features = {}\n",
    "        self.labels = {}\n",
    "        self.subject_ids = []\n",
    "        self.subject_labels = {}\n",
    "        self.bridge_raw_dataset = None\n",
    "        self.bridge_feature_dataset = None\n",
    "        self.eeg_fused_features = {}\n",
    "        self.fmri_fused_features = {}\n",
    "\n",
    "    def set_eeg_features(self, erp, pw, conn):\n",
    "        \"\"\"Store pre-loaded EEG aggregated features.\"\"\"\n",
    "        self.eeg_erp_features = erp\n",
    "        self.eeg_pw_features = pw\n",
    "        self.eeg_conn_features = conn\n",
    "        self.logger.info(f\"EEG features set: ERP={len(erp)}, PW={len(pw)}, CONN={len(conn)}\")\n",
    "\n",
    "    def set_fmri_features(self, act, conn):\n",
    "        \"\"\"Store pre-loaded fMRI features.\"\"\"\n",
    "        self.fmri_act_features = act\n",
    "        self.fmri_conn_features = conn\n",
    "        self.logger.info(f\"fMRI features set: Act={len(act)}, Conn={len(conn)}\")\n",
    "\n",
    "    def set_labels(self, labels):\n",
    "        \"\"\"Set the label dictionary.\"\"\"\n",
    "        self.labels = labels\n",
    "        self.logger.info(f\"Labels set for {len(labels)} subjects\")\n",
    "\n",
    "    def compute_subject_intersection(self):\n",
    "        \"\"\"Compute subjects with complete EEG + fMRI + label data.\"\"\"\n",
    "        eeg_subjs = {int(k[0]) for k in self.eeg_erp_features}\n",
    "        fmri_subjs = set(self.fmri_act_features.keys()) & set(self.fmri_conn_features.keys())\n",
    "        label_subjs = set(self.labels.keys())\n",
    "        common = eeg_subjs & fmri_subjs & label_subjs\n",
    "        self.subject_ids = sorted(common)\n",
    "        self.subject_labels = {s: self.labels[s] for s in self.subject_ids}\n",
    "        label_counts = {}\n",
    "        for v in self.subject_labels.values():\n",
    "            label_counts[v] = label_counts.get(v, 0) + 1\n",
    "        self.logger.info(\n",
    "            f\"Bridge subjects with complete data: {len(self.subject_ids)} \"\n",
    "            f\"(label distribution: {label_counts})\")\n",
    "        return self.subject_ids\n",
    "\n",
    "    def build_raw_dataset(self):\n",
    "        \"\"\"Build the BridgeRawDataset from stored features.\"\"\"\n",
    "        self.bridge_raw_dataset = BridgeRawDataset(\n",
    "            self.eeg_erp_features, self.eeg_pw_features, self.eeg_conn_features,\n",
    "            self.fmri_act_features, self.fmri_conn_features,\n",
    "            self.labels, self.subject_ids,\n",
    "            self.config.bands, self.config.func_segments)\n",
    "        return self.bridge_raw_dataset\n",
    "\n",
    "    def set_fused_features(self, eeg_fused, fmri_fused):\n",
    "        \"\"\"Store extracted fused features from frozen encoders.\"\"\"\n",
    "        self.eeg_fused_features = eeg_fused\n",
    "        self.fmri_fused_features = fmri_fused\n",
    "        self.logger.info(\n",
    "            f\"Fused features set: EEG={len(eeg_fused)}, fMRI={len(fmri_fused)}\")\n",
    "\n",
    "    def build_feature_dataset(self):\n",
    "        \"\"\"Build BridgeFeatureDataset from fused features.\"\"\"\n",
    "        common = sorted(\n",
    "            set(self.eeg_fused_features.keys()) &\n",
    "            set(self.fmri_fused_features.keys()) &\n",
    "            set(self.labels.keys()))\n",
    "        self.bridge_feature_dataset = BridgeFeatureDataset(\n",
    "            self.eeg_fused_features, self.fmri_fused_features,\n",
    "            self.labels, common)\n",
    "        return self.bridge_feature_dataset, common\n",
    "\n",
    "    def get_subject_ids_and_labels(self):\n",
    "        ids = np.array(self.subject_ids)\n",
    "        labs = np.array([self.subject_labels[s] for s in self.subject_ids])\n",
    "        return ids, labs\n",
    "\n",
    "    def get_loso_split_indices(self, dataset, held_out_subject):\n",
    "        \"\"\"Return (train_indices, test_indices) for LOSO fold.\"\"\"\n",
    "        held = int(held_out_subject)\n",
    "        train_idx = [i for i, s in enumerate(dataset.samples)\n",
    "                     if int(s['subject']) != held]\n",
    "        test_idx = [i for i, s in enumerate(dataset.samples)\n",
    "                    if int(s['subject']) == held]\n",
    "        return train_idx, test_idx\n",
    "\n",
    "print(\"BridgeSubjectDataHandler defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize BridgeSubjectDataHandler ---\n",
    "bridge_handler = BridgeSubjectDataHandler(config, logger)\n",
    "bridge_handler.set_eeg_features(eeg_erp_features, eeg_pw_features, eeg_conn_features)\n",
    "bridge_handler.set_fmri_features(fmri_act_features, fmri_conn_features)\n",
    "bridge_handler.set_labels(bridge_labels)\n",
    "bridge_handler.compute_subject_intersection()\n",
    "\n",
    "# Use handler to build raw dataset (replaces standalone BridgeRawDataset call)\n",
    "bridge_raw_dataset = bridge_handler.build_raw_dataset()\n",
    "print(f'\\nAligned subjects: {len(bridge_raw_dataset)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Pre-trained EEG & fMRI Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-trained Models\n",
    "def find_best_checkpoint(checkpoint_dir, pattern):\n",
    "    \"\"\"Find the best checkpoint file matching a glob pattern.\"\"\"\n",
    "    files = sorted(glob.glob(str(Path(checkpoint_dir) / pattern)))\n",
    "    if not files:\n",
    "        logger.warning(f'No checkpoint found matching {checkpoint_dir}/{pattern}')\n",
    "        return None\n",
    "    return files[-1]\n",
    "\n",
    "def load_eeg_model(checkpoint_path, dataset_sample, fusion_dim=128):\n",
    "    \"\"\"Instantiate and load an EEG tri-modal model from checkpoint.\"\"\"\n",
    "    eeg_samples = dataset_sample[0]\n",
    "    sample_erp, sample_pw, sample_conn = eeg_samples[0]\n",
    "    in_erp_dim = sample_erp.shape[0]\n",
    "    in_pw_dim = sample_pw.shape[0]\n",
    "    in_conn_dim = sample_conn.shape[0] if sample_conn.ndim == 1 else np.prod(sample_conn.shape)\n",
    "    logger.info(f'EEG model dims: ERP={in_erp_dim}, PW={in_pw_dim}, CONN={in_conn_dim}')\n",
    "    model = ImprovedTriModalFusionNet(\n",
    "        in_pw_dim=in_pw_dim,\n",
    "        in_erp_dim=in_erp_dim,\n",
    "        in_conn_dim=in_conn_dim,\n",
    "        fusion_dim=fusion_dim,\n",
    "        num_classes=config.num_classes\n",
    "    )\n",
    "    if checkpoint_path:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        logger.info(f'EEG model loaded from {checkpoint_path}')\n",
    "    else:\n",
    "        logger.warning('No EEG checkpoint found, using random weights')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    model.requires_grad_(False)\n",
    "    return model\n",
    "\n",
    "def load_fmri_model(checkpoint_path, fmri_act_dim, fmri_conn_dim, hidden_dim=64):\n",
    "    model = fMRIFusionNet(\n",
    "        activation_dim=fmri_act_dim,\n",
    "        connectivity_dim=fmri_conn_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_classes=config.num_classes\n",
    "    )\n",
    "    if checkpoint_path:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        logger.info(f'fMRI model loaded from {checkpoint_path}')\n",
    "    else:\n",
    "        logger.warning('No fMRI checkpoint found, using random weights')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    model.requires_grad_(False)\n",
    "    return model\n",
    "\n",
    "eeg_ckpt = find_best_checkpoint(config.eeg_checkpoint_dir, 'best_trimodal_fold*.pt')\n",
    "fmri_ckpt = find_best_checkpoint(config.fmri_checkpoint_dir, 'best_fusion_fold*.pt')\n",
    "sample_fmri_act = list(fmri_act_features.values())[0]\n",
    "sample_fmri_conn = list(fmri_conn_features.values())[0]\n",
    "fmri_act_dim = sample_fmri_act.shape[0]\n",
    "fmri_conn_dim = sample_fmri_conn.shape[0]\n",
    "logger.info(f'fMRI dims: activation={fmri_act_dim}, connectivity={fmri_conn_dim}')\n",
    "\n",
    "# Load models\n",
    "sample_data = bridge_raw_dataset[0]\n",
    "eeg_model = load_eeg_model(eeg_ckpt, sample_data, fusion_dim=config.eeg_hidden_dim)\n",
    "fmri_model = load_fmri_model(fmri_ckpt, fmri_act_dim, fmri_conn_dim, hidden_dim=config.fmri_hidden_dim)\n",
    "\n",
    "n_eeg_params = sum(p.numel() for p in eeg_model.parameters())\n",
    "n_fmri_params = sum(p.numel() for p in fmri_model.parameters())\n",
    "logger.info(f'EEG model params: {n_eeg_params:,} (all frozen)')\n",
    "logger.info(f'fMRI model params: {n_fmri_params:,} (all frozen)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Extraction\n",
    "\n",
    "Extract frozen EEG and fMRI features for each subject using the pre-trained encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Functions\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_eeg_features(model, raw_dataset, device):\n",
    "    model.eval()\n",
    "    features = {}\n",
    "    for idx in range(len(raw_dataset)):\n",
    "        eeg_samples, _, _, label, subj = raw_dataset[idx]\n",
    "        feat_list = []\n",
    "        for erp_np, pw_np, conn_np in eeg_samples:\n",
    "            erp_t = torch.tensor(erp_np, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            pw_t = torch.tensor(pw_np, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            conn_t = torch.tensor(conn_np, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            if conn_t.dim() > 2:\n",
    "                conn_t = conn_t.view(conn_t.size(0), -1)\n",
    "            try:\n",
    "                out = model(erp=erp_t, pw=pw_t, conn=conn_t, return_feats=True)\n",
    "                fused = out['fused_feats']\n",
    "                feat_list.append(fused.cpu())\n",
    "            except Exception:\n",
    "                continue\n",
    "        if feat_list:\n",
    "            stacked = torch.cat(feat_list, dim=0)\n",
    "            mean_feat = stacked.mean(dim=0)\n",
    "            features[subj] = mean_feat\n",
    "    logger.info(f'Extracted EEG features for {len(features)} subjects, dim={list(features.values())[0].shape[0] if features else \"N/A\"}')\n",
    "    return features\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_fmri_features(model, fmri_act, fmri_conn, subject_list, device):\n",
    "    \"\"\"Extract fused features from frozen fMRI model.\"\"\"\n",
    "    model.eval()\n",
    "    features = {}\n",
    "    for subj in subject_list:\n",
    "        if subj not in fmri_act or subj not in fmri_conn:\n",
    "            continue\n",
    "        act_t = fmri_act[subj].unsqueeze(0).to(device)\n",
    "        conn_t = fmri_conn[subj].unsqueeze(0).to(device)\n",
    "        try:\n",
    "            _, fused = model(act_t, conn_t, return_features=True)\n",
    "            features[subj] = fused.squeeze(0).cpu()\n",
    "        except Exception as e:\n",
    "            logger.warning(f'fMRI feature extraction failed for subject {subj}: {e}')\n",
    "    logger.info(f'Extracted fMRI features for {len(features)} subjects, dim={list(features.values())[0].shape[0] if features else \"N/A\"}')\n",
    "    return features\n",
    "\n",
    "\n",
    "# Extract features\n",
    "logger.info('Extracting EEG fused features...')\n",
    "eeg_fused_features = extract_eeg_features(eeg_model, bridge_raw_dataset, device)\n",
    "\n",
    "logger.info('Extracting fMRI fused features...')\n",
    "fmri_fused_features = extract_fmri_features(\n",
    "    fmri_model, fmri_act_features, fmri_conn_features, config.overlap_subjects, device\n",
    ")\n",
    "\n",
    "# Verify alignment\n",
    "common_subjects = sorted(set(eeg_fused_features.keys()) & set(fmri_fused_features.keys()) & set(bridge_labels.keys()))\n",
    "logger.info(f'Common subjects with both EEG and fMRI features: {len(common_subjects)}')\n",
    "\n",
    "if common_subjects:\n",
    "    s = common_subjects[0]\n",
    "    logger.info(f'  Sample subject {s}: EEG={eeg_fused_features[s].shape}, fMRI={fmri_fused_features[s].shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Bridge Fusion Model\n",
    "\n",
    "`EEGfMRIBridgeFusionNet` with **LayerNorm** (not BatchNorm) in the classifier\n",
    "for stability with small / single-sample batches during LOOCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bridge Fusion Model (LayerNorm in classifier)\n",
    "\n",
    "class EEGfMRIBridgeFusionNet(nn.Module):\n",
    "    def __init__(self, eeg_dim=128, fmri_dim=64, bridge_dim=128,\n",
    "                 num_classes=2, num_heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bridge_dim = bridge_dim\n",
    "\n",
    "        # Project to shared space\n",
    "        self.eeg_proj = nn.Sequential(\n",
    "            nn.Linear(eeg_dim, bridge_dim),\n",
    "            nn.LayerNorm(bridge_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.fmri_proj = nn.Sequential(\n",
    "            nn.Linear(fmri_dim, bridge_dim),\n",
    "            nn.LayerNorm(bridge_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Cross-modal attention\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            bridge_dim, num_heads=num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Learned fusion with temperature scaling\n",
    "        self.fusion = LearnedFusionModule(\n",
    "            num_modalities=2,\n",
    "            hidden_dim=bridge_dim,\n",
    "            use_temperature=True\n",
    "        )\n",
    "\n",
    "        # Classifier \u00e2\u20ac\u201d LayerNorm for LOOCV compatibility\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bridge_dim, bridge_dim // 2),\n",
    "            nn.LayerNorm(bridge_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(bridge_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, eeg_feats, fmri_feats, return_features=False, return_weights=False):\n",
    "        eeg_proj = self.eeg_proj(eeg_feats)\n",
    "        fmri_proj = self.fmri_proj(fmri_feats)\n",
    "        modality_seq = torch.stack([eeg_proj, fmri_proj], dim=1)\n",
    "\n",
    "        eeg_q = eeg_proj.unsqueeze(1)\n",
    "        attn_out, attn_weights_raw = self.cross_attn(\n",
    "            eeg_q, modality_seq, modality_seq\n",
    "        )\n",
    "        eeg_enhanced = attn_out.squeeze(1)\n",
    "\n",
    "        if return_weights:\n",
    "            fused, fusion_weights = self.fusion(\n",
    "                [eeg_enhanced, fmri_proj], return_weights=True\n",
    "            )\n",
    "        else:\n",
    "            fused = self.fusion([eeg_enhanced, fmri_proj])\n",
    "            fusion_weights = None\n",
    "\n",
    "        logits = self.classifier(fused)\n",
    "\n",
    "        results = [logits]\n",
    "        if return_features:\n",
    "            results.append(fused)\n",
    "        if return_weights:\n",
    "            results.append(fusion_weights)\n",
    "            results.append(attn_weights_raw)\n",
    "\n",
    "        return results[0] if len(results) == 1 else tuple(results)\n",
    "\n",
    "    def get_fusion_weights(self):\n",
    "        with torch.no_grad():\n",
    "            logits = self.fusion.fusion_logits\n",
    "            temp = self.fusion.temperature\n",
    "            weights = F.softmax(logits / temp, dim=0)\n",
    "            return {\n",
    "                'eeg_weight': weights[0].item(),\n",
    "                'fmri_weight': weights[1].item(),\n",
    "                'temperature': temp.item()\n",
    "            }\n",
    "\n",
    "\n",
    "# Quick architecture test\n",
    "_test_bridge = EEGfMRIBridgeFusionNet(\n",
    "    eeg_dim=config.eeg_hidden_dim,\n",
    "    fmri_dim=config.fmri_hidden_dim,\n",
    "    bridge_dim=config.bridge_hidden_dim,\n",
    "    num_classes=config.num_classes,\n",
    "    dropout=config.dropout\n",
    ")\n",
    "n_bridge_params = sum(p.numel() for p in _test_bridge.parameters())\n",
    "n_trainable = sum(p.numel() for p in _test_bridge.parameters() if p.requires_grad)\n",
    "print(f'Bridge model: {n_bridge_params:,} total params, {n_trainable:,} trainable')\n",
    "\n",
    "# Smoke test\n",
    "_eeg_dummy = torch.randn(4, config.eeg_hidden_dim)\n",
    "_fmri_dummy = torch.randn(4, config.fmri_hidden_dim)\n",
    "_logits, _fused, _fw, _aw = _test_bridge(_eeg_dummy, _fmri_dummy, return_features=True, return_weights=True)\n",
    "print(f'Smoke test: logits={_logits.shape}, fused={_fused.shape}, fusion_weights={_fw.shape}, attn_weights={_aw.shape}')\n",
    "del _test_bridge, _eeg_dummy, _fmri_dummy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Bridge Feature Dataset\n",
    "\n",
    "Create the aligned dataset of pre-extracted EEG & fMRI features for LOOCV training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bridge Dataset from Pre-extracted Features\n",
    "\n",
    "class BridgeFeatureDataset(Dataset):\n",
    "    \"\"\"Dataset of pre-extracted EEG and fMRI features, aligned by subject.\"\"\"\n",
    "    def __init__(self, eeg_features, fmri_features, labels, subject_list):\n",
    "        self.samples = []\n",
    "        for subj in sorted(subject_list):\n",
    "            if subj in eeg_features and subj in fmri_features and subj in labels:\n",
    "                self.samples.append({\n",
    "                    'eeg': eeg_features[subj],\n",
    "                    'fmri': fmri_features[subj],\n",
    "                    'label': labels[subj],\n",
    "                    'subject': subj\n",
    "                })\n",
    "        logger.info(f'BridgeFeatureDataset: {len(self.samples)} samples')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        return s['eeg'], s['fmri'], s['label'], s['subject']\n",
    "\n",
    "\n",
    "def collate_bridge(batch):\n",
    "    eeg = torch.stack([b[0] for b in batch])\n",
    "    fmri = torch.stack([b[1] for b in batch])\n",
    "    labels = torch.tensor([b[2] for b in batch], dtype=torch.long)\n",
    "    subjects = [b[3] for b in batch]\n",
    "    return eeg, fmri, labels, subjects\n",
    "\n",
    "\n",
    "bridge_dataset = BridgeFeatureDataset(\n",
    "    eeg_fused_features, fmri_fused_features, bridge_labels, common_subjects\n",
    ")\n",
    "\n",
    "all_labels = np.array([s['label'] for s in bridge_dataset.samples])\n",
    "print(f'Bridge dataset: {len(bridge_dataset)} samples')\n",
    "print(f'Class distribution: {dict(zip(*np.unique(all_labels, return_counts=True)))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Leave-One-Out Cross-Validation (LOOCV) Training\n",
    "\n",
    "Each fold trains on N-1 subjects and tests on 1 held-out subject.\n",
    "XAI results are collected **inside** the LOO loop so that each subject is only\n",
    "analyzed by the model that did **not** train on it (fixing train/test leakage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helpers\n",
    "\n",
    "def train_bridge_epoch(model, loader, optimizer, criterion, device, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for eeg, fmri, labels, _ in loader:\n",
    "        eeg, fmri, labels = eeg.to(device), fmri.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(eeg, fmri)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        if grad_clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(len(loader), 1)\n",
    "\n",
    "\n",
    "def evaluate_bridge(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_targets, all_probs = [], [], []\n",
    "    all_subjects = []\n",
    "    with torch.no_grad():\n",
    "        for eeg, fmri, labels, subjects in loader:\n",
    "            eeg, fmri = eeg.to(device), fmri.to(device)\n",
    "            logits = model(eeg, fmri)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(labels.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_subjects.extend(subjects)\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(all_targets, all_preds),\n",
    "        'F1': f1_score(all_targets, all_preds, average='weighted', zero_division=0),\n",
    "        'Precision': precision_score(all_targets, all_preds, average='weighted', zero_division=0),\n",
    "        'Recall': recall_score(all_targets, all_preds, average='weighted', zero_division=0),\n",
    "    }\n",
    "    try:\n",
    "        metrics['AUC'] = roc_auc_score(all_targets, all_probs[:, 1])\n",
    "    except Exception:\n",
    "        metrics['AUC'] = 0.5\n",
    "    return metrics, all_targets, all_probs, all_subjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOCV Training Loop with per-fold XAI collection\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "n_subjects = len(bridge_dataset)\n",
    "\n",
    "# Aggregated results across all LOO folds\n",
    "loo_predictions = []     # list of (subject, true_label, pred_label, prob_class1)\n",
    "loo_fusion_weights = []  # per-fold fusion weight dicts\n",
    "all_fold_fused_features = {}  # subject -> fused features\n",
    "\n",
    "# Per-subject XAI results (collected from held-out fold only)\n",
    "per_subject_saliency = {}      # subj -> {'eeg': ..., 'fmri': ...}\n",
    "per_subject_ig = {}            # subj -> {'eeg': ..., 'fmri': ...}\n",
    "per_subject_attn_fusion = {}   # subj -> dict\n",
    "\n",
    "logger.info(f'Starting Leave-One-Out CV with {n_subjects} subjects')\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(loo.split(np.zeros(n_subjects)), 1):\n",
    "    test_subj = bridge_dataset.samples[test_idx[0]]['subject']\n",
    "    if fold_idx % 5 == 1:\n",
    "        logger.info(f'LOO fold {fold_idx}/{n_subjects}: held-out subject {test_subj}')\n",
    "\n",
    "    train_subset = Subset(bridge_dataset, train_idx)\n",
    "    test_subset = Subset(bridge_dataset, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=config.batch_size,\n",
    "                              shuffle=True, collate_fn=collate_bridge)\n",
    "    test_loader = DataLoader(test_subset, batch_size=1,\n",
    "                             shuffle=False, collate_fn=collate_bridge)\n",
    "\n",
    "    # Class weights from training set\n",
    "    train_labels = all_labels[train_idx]\n",
    "    cw = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "    cw_tensor = torch.tensor(cw, dtype=torch.float32).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=cw_tensor)\n",
    "\n",
    "    # Create bridge model\n",
    "    bridge_model = EEGfMRIBridgeFusionNet(\n",
    "        eeg_dim=config.eeg_hidden_dim,\n",
    "        fmri_dim=config.fmri_hidden_dim,\n",
    "        bridge_dim=config.bridge_hidden_dim,\n",
    "        num_classes=config.num_classes,\n",
    "        dropout=config.dropout\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(bridge_model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        train_loss = train_bridge_epoch(bridge_model, train_loader, optimizer, criterion, device, config.grad_clip)\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            best_state = copy.deepcopy(bridge_model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= config.patience:\n",
    "            break\n",
    "\n",
    "    # Load best model for this fold\n",
    "    if best_state:\n",
    "        bridge_model.load_state_dict(best_state)\n",
    "\n",
    "    # Evaluate on held-out subject\n",
    "    bridge_model.eval()\n",
    "    with torch.no_grad():\n",
    "        eeg_t, fmri_t, label_t, subj_list = next(iter(test_loader))\n",
    "        eeg_t, fmri_t = eeg_t.to(device), fmri_t.to(device)\n",
    "        logits, fused, fw, aw = bridge_model(eeg_t, fmri_t, return_features=True, return_weights=True)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred = logits.argmax(dim=1).item()\n",
    "        true_label = label_t[0].item()\n",
    "        prob_1 = probs[0, 1].item()\n",
    "\n",
    "        loo_predictions.append((test_subj, true_label, pred, prob_1))\n",
    "        all_fold_fused_features[test_subj] = fused.squeeze(0).cpu()\n",
    "\n",
    "    # Collect fusion weights\n",
    "    fw_dict = bridge_model.get_fusion_weights()\n",
    "    loo_fusion_weights.append(fw_dict)\n",
    "\n",
    "    # --- Per-subject XAI (held-out subject only \u00e2\u20ac\u201d no leakage) ---\n",
    "    # Gradient saliency\n",
    "    eeg_in = eeg_t.clone().detach().requires_grad_(True)\n",
    "    fmri_in = fmri_t.clone().detach().requires_grad_(True)\n",
    "    bridge_model.eval()\n",
    "    logits_xai = bridge_model(eeg_in, fmri_in)\n",
    "    target_cls = logits_xai.argmax(dim=1)\n",
    "    bridge_model.zero_grad()\n",
    "    one_hot = torch.zeros_like(logits_xai)\n",
    "    one_hot.scatter_(1, target_cls.view(-1, 1), 1)\n",
    "    logits_xai.backward(gradient=one_hot)\n",
    "    per_subject_saliency[test_subj] = {\n",
    "        'eeg': eeg_in.grad.abs().cpu().numpy().squeeze(),\n",
    "        'fmri': fmri_in.grad.abs().cpu().numpy().squeeze()\n",
    "    }\n",
    "\n",
    "    # Integrated Gradients\n",
    "    n_steps = 50\n",
    "    eeg_base = torch.zeros_like(eeg_t)\n",
    "    fmri_base = torch.zeros_like(fmri_t)\n",
    "    eeg_diff = eeg_t - eeg_base\n",
    "    fmri_diff = fmri_t - fmri_base\n",
    "    eeg_grads_ig, fmri_grads_ig = [], []\n",
    "    tc = None\n",
    "    for alpha in np.linspace(0, 1, n_steps):\n",
    "        ei = (eeg_base + alpha * eeg_diff).requires_grad_(True)\n",
    "        fi = (fmri_base + alpha * fmri_diff).requires_grad_(True)\n",
    "        lo = bridge_model(ei, fi)\n",
    "        if tc is None:\n",
    "            tc = lo.argmax(dim=1)\n",
    "        bridge_model.zero_grad()\n",
    "        oh = torch.zeros_like(lo)\n",
    "        oh.scatter_(1, tc.view(-1, 1), 1)\n",
    "        lo.backward(gradient=oh)\n",
    "        eeg_grads_ig.append(ei.grad.detach().cpu().numpy())\n",
    "        fmri_grads_ig.append(fi.grad.detach().cpu().numpy())\n",
    "    eeg_ig = eeg_diff.cpu().numpy() * np.mean(eeg_grads_ig, axis=0)\n",
    "    fmri_ig = fmri_diff.cpu().numpy() * np.mean(fmri_grads_ig, axis=0)\n",
    "    per_subject_ig[test_subj] = {\n",
    "        'eeg': np.abs(eeg_ig).squeeze(),\n",
    "        'fmri': np.abs(fmri_ig).squeeze()\n",
    "    }\n",
    "\n",
    "    # Attention & fusion weights\n",
    "    per_subject_attn_fusion[test_subj] = {\n",
    "        'label': true_label,\n",
    "        'prediction': pred,\n",
    "        'fusion_weights': fw.cpu().numpy().squeeze(),\n",
    "        'attn_weights': aw.cpu().numpy().squeeze(),\n",
    "    }\n",
    "\n",
    "# Aggregate LOO results\n",
    "loo_targets = np.array([p[1] for p in loo_predictions])\n",
    "loo_preds = np.array([p[2] for p in loo_predictions])\n",
    "loo_probs = np.array([p[3] for p in loo_predictions])\n",
    "loo_subjects = [p[0] for p in loo_predictions]\n",
    "\n",
    "loo_metrics = {\n",
    "    'Accuracy': accuracy_score(loo_targets, loo_preds),\n",
    "    'F1': f1_score(loo_targets, loo_preds, average='weighted', zero_division=0),\n",
    "    'Precision': precision_score(loo_targets, loo_preds, average='weighted', zero_division=0),\n",
    "    'Recall': recall_score(loo_targets, loo_preds, average='weighted', zero_division=0),\n",
    "}\n",
    "try:\n",
    "    loo_metrics['AUC'] = roc_auc_score(loo_targets, loo_probs)\n",
    "except Exception:\n",
    "    loo_metrics['AUC'] = 0.5\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print('BRIDGE FUSION LOOCV RESULTS')\n",
    "print(f'{\"=\"*60}')\n",
    "for metric, val in loo_metrics.items():\n",
    "    print(f'  {metric:12s}: {val:.4f}')\n",
    "\n",
    "eeg_w = [fw['eeg_weight'] for fw in loo_fusion_weights]\n",
    "fmri_w = [fw['fmri_weight'] for fw in loo_fusion_weights]\n",
    "print(f'\\n  EEG weight:  {np.mean(eeg_w):.4f} +/- {np.std(eeg_w):.4f}')\n",
    "print(f'  fMRI weight: {np.mean(fmri_w):.4f} +/- {np.std(fmri_w):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Results Visualization\n",
    "\n",
    "Single confusion matrix and ROC curve aggregated across all LOO folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Visualization\n",
    "\n",
    "fig_dir = config.output_dir / 'figures'\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Performance Summary Table ---\n",
    "summary_rows = []\n",
    "for metric, val in loo_metrics.items():\n",
    "    summary_rows.append({'Metric': metric, 'Value': val})\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(config.output_dir / f'bridge_summary_{timestamp}.csv', index=False)\n",
    "print('Performance Summary:')\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# --- ROC Curve (single aggregated) ---\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "fpr, tpr, _ = roc_curve(loo_targets, loo_probs)\n",
    "auc_val = loo_metrics['AUC']\n",
    "ax.plot(fpr, tpr, label=f'LOOCV (AUC={auc_val:.3f})', linewidth=2)\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Bridge Fusion ROC Curve (LOOCV)')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / f'roc_curve_loocv_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- Confusion Matrix (single aggregated) ---\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "cm = confusion_matrix(loo_targets, loo_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "ax.set_title('Confusion Matrix (LOOCV)')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / f'confusion_matrix_loocv_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- Fusion Weight Distribution ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].hist(eeg_w, bins=15, alpha=0.7, color='#2ecc71', label='EEG')\n",
    "axes[0].hist(fmri_w, bins=15, alpha=0.7, color='#e74c3c', label='fMRI')\n",
    "axes[0].set_xlabel('Weight')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Fusion Weight Distribution Across LOO Folds')\n",
    "axes[0].legend()\n",
    "\n",
    "bars = axes[1].bar(['EEG', 'fMRI'],\n",
    "                    [np.mean(eeg_w), np.mean(fmri_w)],\n",
    "                    yerr=[np.std(eeg_w), np.std(fmri_w)],\n",
    "                    capsize=10, color=['#2ecc71', '#e74c3c'], edgecolor='black', alpha=0.8)\n",
    "axes[1].set_ylabel('Average Weight')\n",
    "axes[1].set_title('Average Fusion Weights')\n",
    "axes[1].set_ylim(0, 1)\n",
    "for bar, mean in zip(bars, [np.mean(eeg_w), np.mean(fmri_w)]):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                 f'{mean:.3f}', ha='center', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / f'fusion_weights_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- t-SNE of Bridge Fused Features ---\n",
    "if all_fold_fused_features:\n",
    "    from sklearn.manifold import TSNE\n",
    "    feat_subjects = sorted(all_fold_fused_features.keys())\n",
    "    feat_matrix = np.stack([all_fold_fused_features[s].numpy() for s in feat_subjects])\n",
    "    feat_labels = np.array([bridge_labels[s] for s in feat_subjects])\n",
    "\n",
    "    if len(feat_subjects) > 5:\n",
    "        perplexity = min(30, len(feat_subjects) - 1)\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=SEED)\n",
    "        embedded = tsne.fit_transform(feat_matrix)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        for cls in np.unique(feat_labels):\n",
    "            mask = feat_labels == cls\n",
    "            ax.scatter(embedded[mask, 0], embedded[mask, 1],\n",
    "                       label=f'Class {cls}', alpha=0.7, s=60)\n",
    "        ax.set_title('t-SNE of Bridge Fused Features')\n",
    "        ax.set_xlabel('t-SNE 1')\n",
    "        ax.set_ylabel('t-SNE 2')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_dir / f'tsne_bridge_features_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== BRIDGE LOSO SUBJECT-LEVEL METRICS & VISUALIZATION ==========\n",
    "def compute_bridge_loso_subject_metrics(loo_predictions, bridge_labels):\n",
    "    \"\"\"Compute structured per-subject metrics from LOOCV predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    loo_predictions : list of (subject_id, true_label, pred_label, prob_class1)\n",
    "    bridge_labels : dict of subject -> label\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with per-subject results\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for subj, true_lab, pred_lab, prob1 in loo_predictions:\n",
    "        confidence = prob1 if pred_lab == 1 else (1.0 - prob1)\n",
    "        records.append({\n",
    "            'subject_id': int(subj),\n",
    "            'true_label': int(true_lab),\n",
    "            'predicted_label': int(pred_lab),\n",
    "            'prob_class1': prob1,\n",
    "            'confidence': confidence,\n",
    "            'correct': int(true_lab == pred_lab)\n",
    "        })\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    if len(df) > 0:\n",
    "        subj_acc = accuracy_score(df['true_label'], df['predicted_label'])\n",
    "        subj_f1 = f1_score(df['true_label'], df['predicted_label'],\n",
    "                           average='weighted', zero_division=0)\n",
    "        subj_prec = precision_score(df['true_label'], df['predicted_label'],\n",
    "                                     average='weighted', zero_division=0)\n",
    "        subj_rec = recall_score(df['true_label'], df['predicted_label'],\n",
    "                                 average='weighted', zero_division=0)\n",
    "\n",
    "        tn = ((df['true_label'] == 0) & (df['predicted_label'] == 0)).sum()\n",
    "        fp = ((df['true_label'] == 0) & (df['predicted_label'] == 1)).sum()\n",
    "        fn = ((df['true_label'] == 1) & (df['predicted_label'] == 0)).sum()\n",
    "        tp = ((df['true_label'] == 1) & (df['predicted_label'] == 1)).sum()\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"BRIDGE LOSO SUBJECT-LEVEL METRICS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"  Accuracy:    {subj_acc:.4f}\")\n",
    "        print(f\"  F1:          {subj_f1:.4f}\")\n",
    "        print(f\"  Precision:   {subj_prec:.4f}\")\n",
    "        print(f\"  Recall:      {subj_rec:.4f}\")\n",
    "        print(f\"  Sensitivity: {sensitivity:.4f}\")\n",
    "        print(f\"  Specificity: {specificity:.4f}\")\n",
    "\n",
    "        misclassified = df[df['correct'] == 0]\n",
    "        if len(misclassified) > 0:\n",
    "            print(f\"\\nMisclassified subjects ({len(misclassified)}):\")\n",
    "            for _, row in misclassified.iterrows():\n",
    "                print(f\"  Sub-{int(row['subject_id'])}: true={int(row['true_label'])}, \"\n",
    "                      f\"pred={int(row['predicted_label'])}, conf={row['confidence']:.3f}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_bridge_loso_results(df: pd.DataFrame, save_dir='./results_bridge/figures'):\n",
    "    \"\"\"Visualize bridge LOSO subject-level results.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import confusion_matrix as cm_func\n",
    "\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"No results to plot.\")\n",
    "        return\n",
    "\n",
    "    # 1. Per-subject confidence bar chart\n",
    "    fig, ax = plt.subplots(figsize=(14, 5))\n",
    "    df_sorted = df.sort_values('subject_id')\n",
    "    colors = ['#2ecc71' if c == 1 else '#e74c3c' for c in df_sorted['correct']]\n",
    "    ax.bar(range(len(df_sorted)), df_sorted['confidence'], color=colors,\n",
    "           edgecolor='black', linewidth=0.5)\n",
    "    ax.set_xlabel('Subject', fontsize=12)\n",
    "    ax.set_ylabel('Prediction Confidence', fontsize=12)\n",
    "    ax.set_title('Bridge LOSO Per-Subject Prediction Confidence', fontsize=14)\n",
    "    ax.set_xticks(range(len(df_sorted)))\n",
    "    ax.set_xticklabels([f\"S{int(s)}\" for s in df_sorted['subject_id']],\n",
    "                       rotation=90, fontsize=8)\n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='#2ecc71', label='Correct'),\n",
    "                       Patch(facecolor='#e74c3c', label='Incorrect')]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path / 'bridge_loso_subject_confidence.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Subject-level confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    cm = cm_func(df['true_label'], df['predicted_label'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Good (0)', 'Poor (1)'],\n",
    "                yticklabels=['Good (0)', 'Poor (1)'])\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    ax.set_title('Bridge LOSO Subject-Level Confusion Matrix', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path / 'bridge_loso_confusion_matrix.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Confidence distribution\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    correct_conf = df[df['correct'] == 1]['confidence']\n",
    "    incorrect_conf = df[df['correct'] == 0]['confidence']\n",
    "    ax.hist(correct_conf, bins=12, alpha=0.7, color='#2ecc71', label='Correct')\n",
    "    ax.hist(incorrect_conf, bins=12, alpha=0.7, color='#e74c3c', label='Incorrect')\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Confidence Distribution by Correctness')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path / 'bridge_loso_confidence_dist.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    subj_acc = (df['correct'].sum() / len(df)) * 100\n",
    "    print(f\"\\nBridge LOSO Subject-Level Summary:\")\n",
    "    print(f\"  Total subjects: {len(df)}\")\n",
    "    print(f\"  Correct:        {df['correct'].sum()}\")\n",
    "    print(f\"  Incorrect:      {(df['correct'] == 0).sum()}\")\n",
    "    print(f\"  Accuracy:       {subj_acc:.1f}%\")\n",
    "\n",
    "print(\"Bridge LOSO metrics & visualization defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. XAI \u00e2\u20ac\u201d Gradient Saliency Visualization\n",
    "\n",
    "Saliency was computed per held-out subject inside the LOOCV loop (no train/test leakage).\n",
    "Here we aggregate and visualize the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XAI - Gradient Saliency (aggregated from LOOCV)\n",
    "\n",
    "all_eeg_saliency = np.stack([per_subject_saliency[s]['eeg'] for s in loo_subjects])\n",
    "all_fmri_saliency = np.stack([per_subject_saliency[s]['fmri'] for s in loo_subjects])\n",
    "\n",
    "mean_eeg_sal = np.mean(all_eeg_saliency, axis=0)\n",
    "mean_fmri_sal = np.mean(all_fmri_saliency, axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].bar(range(len(mean_eeg_sal)), mean_eeg_sal, color='#2ecc71', alpha=0.7)\n",
    "axes[0].set_title('Gradient Saliency: EEG Features')\n",
    "axes[0].set_xlabel('Feature Dimension')\n",
    "axes[0].set_ylabel('Saliency')\n",
    "\n",
    "axes[1].bar(range(len(mean_fmri_sal)), mean_fmri_sal, color='#e74c3c', alpha=0.7)\n",
    "axes[1].set_title('Gradient Saliency: fMRI Features')\n",
    "axes[1].set_xlabel('Feature Dimension')\n",
    "axes[1].set_ylabel('Saliency')\n",
    "\n",
    "plt.suptitle('Gradient Saliency Analysis (LOOCV held-out)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / f'gradient_saliency_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "total_eeg = np.sum(mean_eeg_sal)\n",
    "total_fmri = np.sum(mean_fmri_sal)\n",
    "total = total_eeg + total_fmri\n",
    "print(f'Gradient saliency modality importance:')\n",
    "print(f'  EEG:  {total_eeg/total:.4f} ({total_eeg:.4f})')\n",
    "print(f'  fMRI: {total_fmri/total:.4f} ({total_fmri:.4f})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. XAI \u00e2\u20ac\u201d Integrated Gradients Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XAI - Integrated Gradients (aggregated from LOOCV)\n",
    "\n",
    "all_eeg_ig = np.stack([per_subject_ig[s]['eeg'] for s in loo_subjects])\n",
    "all_fmri_ig = np.stack([per_subject_ig[s]['fmri'] for s in loo_subjects])\n",
    "\n",
    "mean_eeg_ig = np.mean(all_eeg_ig, axis=0)\n",
    "mean_fmri_ig = np.mean(all_fmri_ig, axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "top_k = 20\n",
    "eeg_top_idx = np.argsort(mean_eeg_ig)[-top_k:][::-1]\n",
    "fmri_top_idx = np.argsort(mean_fmri_ig)[-top_k:][::-1]\n",
    "\n",
    "axes[0].barh(range(top_k), mean_eeg_ig[eeg_top_idx], color='#2ecc71', alpha=0.7)\n",
    "axes[0].set_yticks(range(top_k))\n",
    "axes[0].set_yticklabels([f'EEG-{i}' for i in eeg_top_idx])\n",
    "axes[0].set_title(f'Top {top_k} EEG Feature Attributions (IG)')\n",
    "axes[0].set_xlabel('Attribution')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "axes[1].barh(range(top_k), mean_fmri_ig[fmri_top_idx], color='#e74c3c', alpha=0.7)\n",
    "axes[1].set_yticks(range(top_k))\n",
    "axes[1].set_yticklabels([f'fMRI-{i}' for i in fmri_top_idx])\n",
    "axes[1].set_title(f'Top {top_k} fMRI Feature Attributions (IG)')\n",
    "axes[1].set_xlabel('Attribution')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.suptitle('Integrated Gradients Attribution (LOOCV held-out)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / f'integrated_gradients_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "total_eeg_ig = np.sum(mean_eeg_ig)\n",
    "total_fmri_ig = np.sum(mean_fmri_ig)\n",
    "total_ig = total_eeg_ig + total_fmri_ig\n",
    "print(f'Integrated Gradients modality importance:')\n",
    "print(f'  EEG:  {total_eeg_ig/total_ig:.4f}')\n",
    "print(f'  fMRI: {total_fmri_ig/total_ig:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. XAI \u00e2\u20ac\u201d SHAP Analysis\n",
    "\n",
    "SHAP uses a single model trained on all data for global feature importance.\n",
    "For per-subject SHAP without leakage, the gradient-based methods above (computed\n",
    "inside the LOOCV loop) are preferred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XAI - SHAP Analysis\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print('SHAP not available. Install with: pip install shap')\n",
    "\n",
    "if SHAP_AVAILABLE:\n",
    "    # Train a final model on all data for SHAP global analysis\n",
    "    full_loader = DataLoader(bridge_dataset, batch_size=config.batch_size,\n",
    "                             shuffle=True, collate_fn=collate_bridge)\n",
    "    shap_model = EEGfMRIBridgeFusionNet(\n",
    "        eeg_dim=config.eeg_hidden_dim,\n",
    "        fmri_dim=config.fmri_hidden_dim,\n",
    "        bridge_dim=config.bridge_hidden_dim,\n",
    "        num_classes=config.num_classes,\n",
    "        dropout=config.dropout\n",
    "    ).to(device)\n",
    "\n",
    "    cw_all = compute_class_weight('balanced', classes=np.unique(all_labels), y=all_labels)\n",
    "    cw_all_t = torch.tensor(cw_all, dtype=torch.float32).to(device)\n",
    "    criterion_shap = nn.CrossEntropyLoss(weight=cw_all_t)\n",
    "    opt_shap = optim.AdamW(shap_model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        train_bridge_epoch(shap_model, full_loader, opt_shap, criterion_shap, device, config.grad_clip)\n",
    "\n",
    "    def bridge_predict(inputs):\n",
    "        inputs_t = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "        eeg_part = inputs_t[:, :config.eeg_hidden_dim]\n",
    "        fmri_part = inputs_t[:, config.eeg_hidden_dim:]\n",
    "        shap_model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = shap_model(eeg_part, fmri_part)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "    all_features = []\n",
    "    for idx in range(len(bridge_dataset)):\n",
    "        eeg, fmri, _, _ = bridge_dataset[idx]\n",
    "        combined = torch.cat([eeg, fmri]).numpy()\n",
    "        all_features.append(combined)\n",
    "    all_features = np.array(all_features)\n",
    "\n",
    "    n_background = min(20, len(all_features))\n",
    "    background = all_features[:n_background]\n",
    "\n",
    "    explainer = shap.KernelExplainer(bridge_predict, background)\n",
    "    shap_values = explainer.shap_values(all_features, nsamples=100)\n",
    "\n",
    "    if isinstance(shap_values, list):\n",
    "        sv = shap_values[1]\n",
    "    else:\n",
    "        sv = shap_values\n",
    "\n",
    "    eeg_shap = sv[:, :config.eeg_hidden_dim]\n",
    "    fmri_shap = sv[:, config.eeg_hidden_dim:]\n",
    "\n",
    "    feature_names = ([f'EEG-{i}' for i in range(config.eeg_hidden_dim)] +\n",
    "                     [f'fMRI-{i}' for i in range(config.fmri_hidden_dim)])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    shap.summary_plot(sv, all_features, feature_names=feature_names,\n",
    "                      max_display=20, show=False)\n",
    "    plt.title('SHAP Feature Importance (Top 20)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_dir / f'shap_summary_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    eeg_importance = np.mean(np.abs(eeg_shap))\n",
    "    fmri_importance = np.mean(np.abs(fmri_shap))\n",
    "    total_shap = eeg_importance + fmri_importance\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    bars = ax.bar(['EEG', 'fMRI'],\n",
    "                  [eeg_importance/total_shap, fmri_importance/total_shap],\n",
    "                  color=['#2ecc71', '#e74c3c'], edgecolor='black', alpha=0.8)\n",
    "    ax.set_ylabel('Relative SHAP Importance')\n",
    "    ax.set_title('SHAP Modality Importance')\n",
    "    for bar in bars:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{bar.get_height():.3f}', ha='center', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_dir / f'shap_modality_importance_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'SHAP modality importance: EEG={eeg_importance/total_shap:.4f}, fMRI={fmri_importance/total_shap:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. XAI \u00e2\u20ac\u201d Attention & Fusion Weight Analysis\n",
    "\n",
    "Per-subject attention and fusion weights collected from LOOCV held-out predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XAI - Attention & Fusion Weight Analysis (from LOOCV)\n",
    "\n",
    "subject_xai = []\n",
    "for subj in loo_subjects:\n",
    "    d = per_subject_attn_fusion[subj]\n",
    "    d['subject'] = subj\n",
    "    subject_xai.append(d)\n",
    "\n",
    "# --- Attention Heatmap ---\n",
    "attn_matrix = np.stack([s['attn_weights'] for s in subject_xai])\n",
    "if attn_matrix.ndim == 4:\n",
    "    mean_attn = np.mean(attn_matrix, axis=(0, 1))\n",
    "elif attn_matrix.ndim == 3:\n",
    "    mean_attn = np.mean(attn_matrix, axis=0)\n",
    "else:\n",
    "    mean_attn = attn_matrix.reshape(-1, 2).mean(axis=0, keepdims=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "sns.heatmap(mean_attn.reshape(1, -1), annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "            xticklabels=['EEG', 'fMRI'], yticklabels=['Query'], ax=ax)\n",
    "ax.set_title('Cross-Modal Attention Weights (Mean, LOOCV)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / f'attention_heatmap_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- Per-Subject Fusion Weights ---\n",
    "fusion_weights_arr = np.stack([s['fusion_weights'] for s in subject_xai])\n",
    "if fusion_weights_arr.ndim == 1:\n",
    "    fusion_weights_arr = fusion_weights_arr.reshape(-1, 2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "subjects_list = [s['subject'] for s in subject_xai]\n",
    "x = np.arange(len(subjects_list))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, fusion_weights_arr[:, 0], width, label='EEG', color='#2ecc71', alpha=0.8)\n",
    "ax.bar(x + width/2, fusion_weights_arr[:, 1], width, label='fMRI', color='#e74c3c', alpha=0.8)\n",
    "ax.set_xlabel('Subject')\n",
    "ax.set_ylabel('Fusion Weight')\n",
    "ax.set_title('Per-Subject Dynamic Fusion Weights (LOOCV)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(subjects_list, rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / f'per_subject_fusion_weights_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- Class-wise Fusion Weight Comparison ---\n",
    "class0_mask = np.array([s['label'] == 0 for s in subject_xai])\n",
    "class1_mask = np.array([s['label'] == 1 for s in subject_xai])\n",
    "\n",
    "print('Fusion weights by class:')\n",
    "if class0_mask.any():\n",
    "    c0_eeg = fusion_weights_arr[class0_mask, 0].mean()\n",
    "    c0_fmri = fusion_weights_arr[class0_mask, 1].mean()\n",
    "    print(f'  Class 0: EEG={c0_eeg:.4f}, fMRI={c0_fmri:.4f}')\n",
    "if class1_mask.any():\n",
    "    c1_eeg = fusion_weights_arr[class1_mask, 0].mean()\n",
    "    c1_fmri = fusion_weights_arr[class1_mask, 1].mean()\n",
    "    print(f'  Class 1: EEG={c1_eeg:.4f}, fMRI={c1_fmri:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Summary & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Summary & Export (with structured subject metrics) ==========\n",
    "\n",
    "print('=' * 70)\n",
    "print('EEG-fMRI BRIDGE FUSION - FINAL SUMMARY')\n",
    "print('=' * 70)\n",
    "\n",
    "print(f'\\nDataset: {len(bridge_dataset)} subjects (overlap of EEG & fMRI)')\n",
    "print(f'Cross-validation: Leave-One-Out ({n_subjects} folds)')\n",
    "\n",
    "print(f'\\nBridge Fusion Performance:')\n",
    "for metric, val in loo_metrics.items():\n",
    "    print(f'  {metric:12s}: {val:.4f}')\n",
    "\n",
    "print(f'\\nLearned Fusion Weights:')\n",
    "print(f'  EEG:  {np.mean(eeg_w):.4f} +/- {np.std(eeg_w):.4f}')\n",
    "print(f'  fMRI: {np.mean(fmri_w):.4f} +/- {np.std(fmri_w):.4f}')\n",
    "\n",
    "# --- Structured per-subject metrics ---\n",
    "loso_subject_df = compute_bridge_loso_subject_metrics(loo_predictions, bridge_labels)\n",
    "loso_subject_df.to_csv(config.output_dir / f'bridge_loso_subject_results_{timestamp}.csv', index=False)\n",
    "print(f'\\nPer-subject results saved to {config.output_dir / f\"bridge_loso_subject_results_{timestamp}.csv\"}')\n",
    "\n",
    "# Visualize per-subject results\n",
    "plot_bridge_loso_results(loso_subject_df, save_dir=str(config.output_dir / 'figures'))\n",
    "\n",
    "# Store fused features in handler\n",
    "bridge_handler.set_fused_features(eeg_fused_features, fmri_fused_features)\n",
    "\n",
    "# Per-subject predictions (original format)\n",
    "pred_df = pd.DataFrame(loo_predictions, columns=['Subject', 'True', 'Predicted', 'Prob_Class1'])\n",
    "pred_df.to_csv(config.output_dir / f'bridge_loocv_predictions_{timestamp}.csv', index=False)\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(config.output_dir / f'bridge_summary_{timestamp}.csv', index=False)\n",
    "\n",
    "# Save fusion weights\n",
    "fw_df = pd.DataFrame(loo_fusion_weights)\n",
    "fw_df.to_csv(config.output_dir / f'bridge_fusion_weights_{timestamp}.csv', index=False)\n",
    "\n",
    "subj_fw_records = [\n",
    "    {'subject': s['subject'], 'label': s['label'],\n",
    "     'eeg_weight': float(s['fusion_weights'][0]),\n",
    "     'fmri_weight': float(s['fusion_weights'][1])}\n",
    "    for s in subject_xai\n",
    "]\n",
    "subj_fw_df = pd.DataFrame(subj_fw_records)\n",
    "subj_fw_df.to_csv(config.output_dir / f'bridge_subject_fusion_weights_{timestamp}.csv', index=False)\n",
    "\n",
    "# Save XAI arrays\n",
    "np.savez(\n",
    "    config.output_dir / f'bridge_xai_arrays_{timestamp}.npz',\n",
    "    gradient_saliency_eeg=mean_eeg_sal,\n",
    "    gradient_saliency_fmri=mean_fmri_sal,\n",
    "    integrated_gradients_eeg=mean_eeg_ig,\n",
    "    integrated_gradients_fmri=mean_fmri_ig,\n",
    "    per_subject_fusion_weights=fusion_weights_arr,\n",
    ")\n",
    "\n",
    "logger.info(f'All results saved to {config.output_dir}')\n",
    "logger.info(f'Figures saved to {fig_dir}')\n",
    "print(f'\\nOutput directory: {config.output_dir}')\n",
    "print(f'Figures directory: {fig_dir}')\n",
    "print(f'Timestamp: {timestamp}')\n",
    "print('\\nBridge fusion pipeline complete.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}