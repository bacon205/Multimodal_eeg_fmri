{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Configuration\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import copy\n",
    "import h5py\n",
    "import logging\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,\n",
    "                              roc_auc_score, confusion_matrix, roc_curve)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add pipeline directories to path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'EEG_CODE'))\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'fMRI_CODE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BridgeConfig:\n",
    "    def __init__(self):\n",
    "        self.eeg_checkpoint_dir = Path('./EEG_CODE/checkpoints')\n",
    "        self.fmri_checkpoint_dir = Path('./fMRI_CODE/checkpoints_fmri')\n",
    "        self.eeg_hidden_dim = 128\n",
    "        self.fmri_hidden_dim = 64\n",
    "        self.bridge_hidden_dim = 128\n",
    "        self.num_classes = 2\n",
    "        self.overlap_subjects = list(range(1, 33))\n",
    "        self.n_splits = 5\n",
    "        self.batch_size = 8\n",
    "        self.num_epochs = 50\n",
    "        self.lr = 1e-4\n",
    "        self.weight_decay = 1e-4\n",
    "        self.patience = 10\n",
    "        self.grad_clip = 1.0\n",
    "        self.dropout = 0.3\n",
    "        #self.eeg_base_path = Path(os.getenv('EEG_DATA_PATH', r'E:\\Intermediate\\BACON_ERIC\\Head_neck'      ))\n",
    "        self.eeg_base_path = Path(os.getenv('EEG_DATA_PATH', r'E:\\Head_neck'))\n",
    "        self.eeg_path_pw = self.eeg_base_path / 'EEG' / 'DATA' / 'PROC' / 'data_proc' / 'cleaned_data' / 'TF_dir' / 'pwspctrm' / 'PWS' / 'feat' / 'New'\n",
    "        self.eeg_path_erp = self.eeg_base_path / 'EEG' / 'DATA' / 'PROC' / 'data_proc' / 'cleaned_data' / 'TF_dir' / 'ERP' / 'New'\n",
    "        self.eeg_path_conn = self.eeg_base_path / 'EEG' / 'DATA' / 'PROC' / 'data_proc' / 'cleaned_data' / 'conn_dir' / 'New'\n",
    "        self.eeg_label_path = self.eeg_base_path / 'EEG' / 'DATA' / 'PROC' / 'data_proc' / 'cleaned_data' / 'TF_dir'\n",
    "        #self.fmri_base_path = Path(r'E:\\Intermediate\\BACON_ERIC\\Head_neck\\fMRI\\Neck-Tumor_data\\PATIENTS')\n",
    "        self.fmri_base_path = Path(r'E:\\Head_neck\\fMRI')\n",
    "        self.fmri_data_dir = self.fmri_base_path\n",
    "        self.fmri_label_path = self.fmri_base_path / 'DATA' / 'labels'\n",
    "        self.fmri_activation_types = ['sensory', 'AN', 'LN', 'cognitive', 'DMN']\n",
    "        self.fmri_connectivity_types = ['DMN']\n",
    "        self.fmri_agg_method = 'both'\n",
    "        self.bands = {'alpha': 'Alpha', 'beta': 'Beta', 'theta': 'Theta'}\n",
    "        self.eeg_segments = ['1_Hz', '2_Hz', '4_Hz', '6_Hz', '8_Hz', '10_Hz', '12_Hz',\n",
    "                             '14_Hz', '16_Hz', '18_Hz', '20_Hz', '25_Hz', '30_Hz', '40_Hz']\n",
    "        self.func_segments = ['open', 'close']\n",
    "        self.output_dir = Path('./results_bridge')\n",
    "        self.checkpoint_dir = Path('./checkpoints_bridge')\n",
    "        self.log_dir = Path('./logs_bridge')\n",
    "        for d in [self.output_dir, self.checkpoint_dir, self.log_dir]:\n",
    "            d.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-17 03:01:02,680 - bridge_fusion - INFO - Device: cpu\n",
      "2026-02-17 03:01:02,680 - bridge_fusion - INFO - Bridge Config: overlap_subjects=32, bridge_hidden_dim=128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 1 complete: Imports & configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "config = BridgeConfig()\n",
    "log_file = config.log_dir / 'bridge_fusion.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler(log_file), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger('bridge_fusion')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "logger.info(f'Device: {device}')\n",
    "logger.info(f'Bridge Config: overlap_subjects={len(config.overlap_subjects)}, '\n",
    "            f'bridge_hidden_dim={config.bridge_hidden_dim}')\n",
    "print('Cell 1 complete: Imports & configuration loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CrossModal V4 Enhancements Module Loaded\n",
      "============================================================\n",
      "\n",
      "Available classes:\n",
      "  - EnhancedTriModalFusionNetV4 (tri-modal: ERP + PW + Connectivity)\n",
      "  - EnhancedSmartFusionNetV4 (bi-modal: ERP + PW, with cross-attention)\n",
      "  - BiDirectionalCrossAttention (NEW: mutual cross-modal attention)\n",
      "  - EnhancedERPEncoder (1D CNN + temporal transformers)\n",
      "  - EnhancedPowerEncoder (multi-scale CNN + transformers)\n",
      "  - LearnedFusionModule (learned fusion weights with temperature)\n",
      "\n",
      "Utility functions:\n",
      "  - get_fusion_weights_from_model(model)\n",
      "  - count_parameters(model)\n",
      "\n",
      "V4.1 Updates:\n",
      "  - Bi-modal now has bi-directional cross-modal attention\n",
      "  - Increased dropout (0.3 -> 0.4) for small datasets\n",
      "  - Deeper classifier head matching tri-modal\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "V4-LITE Components for Small Datasets Added\n",
      "============================================================\n",
      "\n",
      "New classes for improved trimodal performance:\n",
      "  - EnhancedTriModalFusionNetV4Lite (lightweight, ~400K params)\n",
      "  - BalancedTriModalDataset (handles sample count mismatch)\n",
      "  - HybridFusionModule (early ERP-PW + late CONN fusion)\n",
      "  - EnhancedConnEncoder (with feature attention)\n",
      "  - LiteERPEncoder, LitePowerEncoder (no transformers)\n",
      "\n",
      "Training utilities:\n",
      "  - CosineAnnealingWarmup (LR scheduler)\n",
      "  - EarlyStopping (prevent overfitting)\n",
      "  - LabelSmoothingCrossEntropy\n",
      "  - DropPath (stochastic depth)\n",
      "\n",
      "Recommended for datasets with < 500 subjects\n",
      "============================================================\n",
      "\n",
      "Cell 2 complete: Pipeline model architectures defined\n"
     ]
    }
   ],
   "source": [
    "# Import shared EEG components\n",
    "from crossmodal_v4_enhancements import (\n",
    "    EnhancedTriModalFusionNetV4,\n",
    "    LearnedFusionModule,\n",
    "    EnhancedERPEncoder,\n",
    "    EnhancedPowerEncoder,\n",
    "    get_fusion_weights_from_model\n",
    ")\n",
    "\n",
    "\n",
    "# ---- EEG Model Wrapper (matches checkpoint structure) ----\n",
    "class ImprovedTriModalFusionNet(nn.Module):\n",
    "    \"\"\"Wrapper around EnhancedTriModalFusionNetV4 matching EEG checkpoint keys.\"\"\"\n",
    "    def __init__(self, in_pw_dim, in_erp_dim, in_conn_dim,\n",
    "                 fusion_dim=128, num_classes=2, dropout=0.3,\n",
    "                 num_transformer_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.model = EnhancedTriModalFusionNetV4(\n",
    "            erp_channels=in_erp_dim,\n",
    "            pw_channels=in_pw_dim,\n",
    "            conn_features=in_conn_dim,\n",
    "            hidden_dim=fusion_dim,\n",
    "            num_classes=num_classes,\n",
    "            dropout=dropout,\n",
    "            num_transformer_layers=num_transformer_layers,\n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        self.fusion_weight_history = []\n",
    "\n",
    "    def forward(self, erp, pw, conn, return_feats=False):\n",
    "        if return_feats:\n",
    "            logits, fusion_weights, fused_feats = self.model(\n",
    "                erp, pw, conn,\n",
    "                return_fusion_weights=True,\n",
    "                return_fused_feats=True\n",
    "            )\n",
    "            return {\n",
    "                'logits': logits,\n",
    "                'gates': fusion_weights,\n",
    "                'fused_feats': fused_feats\n",
    "            }\n",
    "        else:\n",
    "            return self.model(erp, pw, conn)\n",
    "\n",
    "    def get_fusion_weights(self):\n",
    "        return get_fusion_weights_from_model(self.model)\n",
    "\n",
    "\n",
    "# ---- fMRI Model Components (matching checkpoint structure) ----\n",
    "class ActivationEncoder(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int = 64, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class ConnectivityEncoderFMRI(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int = 64, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class fMRIFusionNet(nn.Module):\n",
    "    \"\"\"fMRI fusion model matching checkpoint structure. Bug-fixed forward.\"\"\"\n",
    "    def __init__(self, activation_dim: int, connectivity_dim: int, hidden_dim: int = 64,\n",
    "                 num_classes: int = 2, dropout: float = 0.4, task: str = 'classification'):\n",
    "        super().__init__()\n",
    "        self.task = task\n",
    "        self.activation_encoder = ActivationEncoder(activation_dim, hidden_dim, dropout)\n",
    "        self.connectivity_encoder = ConnectivityEncoderFMRI(connectivity_dim, hidden_dim, dropout)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.activation_weight = nn.Parameter(torch.ones(1) * 0.5)\n",
    "        self.connectivity_weight = nn.Parameter(torch.ones(1) * 0.5)\n",
    "        if task == 'classification':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim // 2, num_classes)\n",
    "            )\n",
    "        else:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim // 2, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, activation, connectivity, return_features=False):\n",
    "        act_feat = self.activation_encoder(activation)\n",
    "        conn_feat = self.connectivity_encoder(connectivity)\n",
    "        weights = F.softmax(torch.stack([self.activation_weight, self.connectivity_weight]), dim=0)\n",
    "        weighted_act = act_feat * weights[0]\n",
    "        weighted_conn = conn_feat * weights[1]\n",
    "        combined = torch.cat([weighted_act, weighted_conn], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        output = self.head(fused)\n",
    "        if self.task == 'regression':\n",
    "            output = output.squeeze(-1)\n",
    "        if return_features:\n",
    "            return output, fused\n",
    "        return output\n",
    "\n",
    "    def get_fusion_weights(self):\n",
    "        with torch.no_grad():\n",
    "            weights = F.softmax(torch.stack([self.activation_weight, self.connectivity_weight]), dim=0)\n",
    "            return {'activation': weights[0].item(), 'connectivity': weights[1].item()}\n",
    "\n",
    "\n",
    "print('Cell 2 complete: Pipeline model architectures defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-17 03:01:02,784 - bridge_fusion - INFO - Loading EEG data for subjects 1-32...\n",
      "2026-02-17 03:01:02,891 - bridge_fusion - INFO - EEG labels: 66 subjects\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Loading - EEG Side\n",
    "# Re-implements loading logic from CrossModal_script_01.ipynb for subjects 1-32\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_eeg_conn_features(conn_dir, subject_list, band_list, cond_list):\n",
    "    \"\"\"Load EEG connectivity features from .mat files.\"\"\"\n",
    "    conn_features = {}\n",
    "    for subj in subject_list:\n",
    "        subj_str = f'{subj:02d}'\n",
    "        for band_key, band_name in band_list.items():\n",
    "            for cond in cond_list:\n",
    "                pattern = conn_dir / f'conn_{band_name}_{cond}_sub{subj_str}.mat'\n",
    "                files = sorted(glob.glob(str(pattern)))\n",
    "                if not files:\n",
    "                    pattern_lower = conn_dir / f'conn_{band_key}_{cond}_sub{subj_str}.mat'\n",
    "                    files = sorted(glob.glob(str(pattern_lower)))\n",
    "                for f in files:\n",
    "                    try:\n",
    "                        mat = loadmat(f)\n",
    "                        for k in mat:\n",
    "                            if not k.startswith('_'):\n",
    "                                data = np.array(mat[k], dtype=np.float32).flatten()\n",
    "                                data = np.nan_to_num(data, nan=0.0)\n",
    "                                label_val = 0  # placeholder\n",
    "                                conn_key = (subj, band_key, cond, label_val)\n",
    "                                conn_features[conn_key] = data\n",
    "                                break\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f'Error loading {f}: {e}')\n",
    "    logger.info(f'Loaded {len(conn_features)} EEG connectivity samples')\n",
    "    return conn_features\n",
    "\n",
    "\n",
    "def load_eeg_pw_features(pw_dir, subject_list, band_list, freq_list):\n",
    "    \"\"\"Load EEG power spectrum features from .mat files.\"\"\"\n",
    "    pw_features = {}\n",
    "    for subj in subject_list:\n",
    "        subj_str = f'{subj:02d}'\n",
    "        for band in band_list:\n",
    "            for freq in freq_list:\n",
    "                pattern = str(pw_dir / f'powspctrm_{band}_{freq}_sub{subj_str}.mat')\n",
    "                for f in sorted(glob.glob(pattern)):\n",
    "                    try:\n",
    "                        mat = loadmat(f)\n",
    "                        for k in mat:\n",
    "                            if not k.startswith('_'):\n",
    "                                data = np.array(mat[k], dtype=np.float32).flatten()\n",
    "                                data = np.nan_to_num(data, nan=0.0)\n",
    "                                label_val = 0\n",
    "                                pw_key = (subj, band, freq, label_val)\n",
    "                                pw_features[pw_key] = data\n",
    "                                break\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f'Error loading {f}: {e}')\n",
    "    logger.info(f'Loaded {len(pw_features)} EEG power spectrum samples')\n",
    "    return pw_features\n",
    "\n",
    "\n",
    "def load_eeg_erp_features(erp_dir, subject_list, band_list, freq_list):\n",
    "    \"\"\"Load EEG ERP features from .mat/.h5 files.\"\"\"\n",
    "    erp_features = {}\n",
    "    for subj in subject_list:\n",
    "        subj_str = f'{subj:02d}'\n",
    "        for band in band_list:\n",
    "            for freq in freq_list:\n",
    "                pattern = erp_dir / f'ERP_sub{subj_str}_{band}_{freq}*.mat'\n",
    "                erp_files = sorted(glob.glob(str(pattern)))\n",
    "                for f in erp_files:\n",
    "                    try:\n",
    "                        with h5py.File(f, 'r') as hf:\n",
    "                            # Navigate to ERP data\n",
    "                            if 'erp_struct' in hf:\n",
    "                                erp_group = hf['erp_struct']\n",
    "                            elif 'erp' in hf:\n",
    "                                erp_group = hf['erp']\n",
    "                            else:\n",
    "                                erp_group = hf[list(hf.keys())[0]]\n",
    "\n",
    "                            if 'avg' in erp_group:\n",
    "                                data = np.array(erp_group['avg'], dtype=np.float32)\n",
    "                            elif 'trial' in erp_group:\n",
    "                                data = np.array(erp_group['trial'], dtype=np.float32)\n",
    "                                if data.ndim == 3:\n",
    "                                    data = np.mean(data, axis=0)\n",
    "                            else:\n",
    "                                # Try first dataset-like key\n",
    "                                for dk in erp_group.keys():\n",
    "                                    candidate = erp_group[dk]\n",
    "                                    if hasattr(candidate, 'shape') and len(candidate.shape) >= 2:\n",
    "                                        data = np.array(candidate, dtype=np.float32)\n",
    "                                        break\n",
    "                                else:\n",
    "                                    continue\n",
    "\n",
    "                            data = np.nan_to_num(data, nan=0.0)\n",
    "                            label_val = 0\n",
    "                            erp_key = (subj, band, freq, label_val)\n",
    "                            erp_features[erp_key] = data\n",
    "                    except Exception as e:\n",
    "                        # Try scipy loadmat fallback\n",
    "                        try:\n",
    "                            mat = loadmat(f)\n",
    "                            for k in mat:\n",
    "                                if not k.startswith('_'):\n",
    "                                    data = np.array(mat[k], dtype=np.float32)\n",
    "                                    data = np.nan_to_num(data, nan=0.0)\n",
    "                                    label_val = 0\n",
    "                                    erp_key = (subj, band, freq, label_val)\n",
    "                                    erp_features[erp_key] = data\n",
    "                                    break\n",
    "                        except Exception:\n",
    "                            logger.warning(f'Error loading ERP {f}: {e}')\n",
    "    logger.info(f'Loaded {len(erp_features)} EEG ERP samples')\n",
    "    return erp_features\n",
    "\n",
    "\n",
    "def load_eeg_labels(label_dir, binary=True):\n",
    "    \"\"\"Load EEG clinical labels.\"\"\"\n",
    "    csv_path = os.path.join(label_dir, 'medical_score.csv')\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f'Label file not found: {csv_path}')\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=['Postoperative evaluation'])\n",
    "    if df['Subject'].dtype == object:\n",
    "        df['subject_id'] = df['Subject'].str.replace('sub', '', regex=False).astype(int)\n",
    "    else:\n",
    "        df['subject_id'] = df['Subject'].astype(int)\n",
    "    label_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        subj = int(row['subject_id'])\n",
    "        score = row['Postoperative evaluation']\n",
    "        label_dict[subj] = 0 if score <= 2 else 1 if binary else score\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "# Load EEG data (filtered to subjects 1-32)\n",
    "logger.info('Loading EEG data for subjects 1-32...')\n",
    "\n",
    "eeg_label_dict = load_eeg_labels(str(config.eeg_label_path))\n",
    "logger.info(f'EEG labels: {len(eeg_label_dict)} subjects')\n",
    "\n",
    "band_keys = list(config.bands.keys())\n",
    "\n",
    "eeg_erp_features = load_eeg_erp_features(\n",
    "    config.eeg_path_erp, config.overlap_subjects, band_keys, config.eeg_segments\n",
    ")\n",
    "eeg_pw_features = load_eeg_pw_features(\n",
    "    config.eeg_path_pw, config.overlap_subjects, band_keys, config.eeg_segments\n",
    ")\n",
    "eeg_conn_features = load_eeg_conn_features(\n",
    "    config.eeg_path_conn, config.overlap_subjects, config.bands, config.func_segments\n",
    ")\n",
    "\n",
    "logger.info(f'EEG data loaded: ERP={len(eeg_erp_features)}, PW={len(eeg_pw_features)}, CONN={len(eeg_conn_features)}')\n",
    "print('Cell 3 complete: EEG data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Data Loading - fMRI Side\n",
    "\n",
    "def load_fmri_activation_features(data_dir, subject_list, activation_types, agg_method='both'):\n",
    "    \"\"\"Load fMRI activation features.\"\"\"\n",
    "    features = {}\n",
    "    for subj in tqdm(subject_list, desc='Loading fMRI activations'):\n",
    "        subj_features = []\n",
    "        subj_dir = data_dir / f'sub-{subj}'\n",
    "        for act_type in activation_types:\n",
    "            filepath = subj_dir / f'subject_{subj}_activation_{act_type}.csv'\n",
    "            if not filepath.exists():\n",
    "                continue\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                if 'Subject' in df.columns:\n",
    "                    df = df.drop('Subject', axis=1)\n",
    "                data = df.values.astype(np.float32)\n",
    "                data = np.nan_to_num(data, nan=0.0)\n",
    "                if agg_method == 'mean':\n",
    "                    agg_data = np.mean(data, axis=0)\n",
    "                elif agg_method == 'std':\n",
    "                    agg_data = np.std(data, axis=0)\n",
    "                elif agg_method == 'both':\n",
    "                    agg_data = np.concatenate([np.mean(data, axis=0), np.std(data, axis=0)])\n",
    "                else:\n",
    "                    raise ValueError(f'Unknown agg method: {agg_method}')\n",
    "                subj_features.append(agg_data)\n",
    "            except Exception as e:\n",
    "                logger.warning(f'Error loading {filepath}: {e}')\n",
    "        if subj_features:\n",
    "            features[subj] = torch.tensor(np.concatenate(subj_features), dtype=torch.float32)\n",
    "    logger.info(f'fMRI activation features: {len(features)} subjects')\n",
    "    if features:\n",
    "        sample = list(features.values())[0]\n",
    "        logger.info(f'  Activation feature dim: {sample.shape[0]}')\n",
    "    return features\n",
    "\n",
    "\n",
    "def load_fmri_connectivity_features(data_dir, subject_list, connectivity_types):\n",
    "    \"\"\"Load fMRI connectivity features.\"\"\"\n",
    "    features = {}\n",
    "    for subj in tqdm(subject_list, desc='Loading fMRI connectivity'):\n",
    "        subj_features = []\n",
    "        subj_dir = data_dir / f'sub-{subj}'\n",
    "        for conn_type in connectivity_types:\n",
    "            filepath = subj_dir / f'subject_{subj}_fdr_PPI_Connectivity_{conn_type}.csv'\n",
    "            if not filepath.exists():\n",
    "                continue\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                if 'Subject' in df.columns:\n",
    "                    df = df.drop('Subject', axis=1)\n",
    "                data = df.values.astype(np.float32).flatten()\n",
    "                data = np.nan_to_num(data, nan=0.0)\n",
    "                subj_features.append(data)\n",
    "            except Exception as e:\n",
    "                logger.warning(f'Error loading {filepath}: {e}')\n",
    "        if subj_features:\n",
    "            features[subj] = torch.tensor(np.concatenate(subj_features), dtype=torch.float32)\n",
    "    logger.info(f'fMRI connectivity features: {len(features)} subjects')\n",
    "    if features:\n",
    "        sample = list(features.values())[0]\n",
    "        logger.info(f'  Connectivity feature dim: {sample.shape[0]}')\n",
    "    return features\n",
    "\n",
    "\n",
    "def load_fmri_labels(label_path, subject_list):\n",
    "    \"\"\"Load fMRI labels.\"\"\"\n",
    "    label_files = [label_path / 'labels.csv', label_path / 'outcomes.csv',\n",
    "                   label_path / 'subjects_labels.csv', label_path.parent / 'labels.csv']\n",
    "    label_file = None\n",
    "    for lf in label_files:\n",
    "        if lf.exists():\n",
    "            label_file = lf\n",
    "            break\n",
    "    if label_file is None:\n",
    "        logger.warning('No fMRI label file found. Using dummy labels.')\n",
    "        return {subj: np.random.randint(0, 2) for subj in subject_list}\n",
    "\n",
    "    df = pd.read_csv(label_file)\n",
    "    subj_col = next((c for c in ['Subject', 'subject', 'SubjectID', 'ID', 'id'] if c in df.columns), None)\n",
    "    label_col = next((c for c in ['Label', 'label', 'Outcome', 'outcome', 'Class', 'class', 'Group', 'group'] if c in df.columns), None)\n",
    "    if not subj_col or not label_col:\n",
    "        raise ValueError(f'Cannot identify columns in {label_file}: {df.columns.tolist()}')\n",
    "\n",
    "    class_labels = {}\n",
    "    for _, row in df.iterrows():\n",
    "        subj = int(row[subj_col])\n",
    "        if subj not in subject_list:\n",
    "            continue\n",
    "        label = row[label_col]\n",
    "        if isinstance(label, str):\n",
    "            label = 1 if label.lower() in ['good', 'positive', 'yes', '1'] else 0\n",
    "        else:\n",
    "            label = int(label)\n",
    "        class_labels[subj] = label\n",
    "    logger.info(f'fMRI labels: {len(class_labels)} subjects, classes={set(class_labels.values())}')\n",
    "    return class_labels\n",
    "\n",
    "\n",
    "# Load fMRI data\n",
    "logger.info('Loading fMRI data...')\n",
    "\n",
    "fmri_act_features = load_fmri_activation_features(\n",
    "    config.fmri_data_dir, config.overlap_subjects,\n",
    "    config.fmri_activation_types, config.fmri_agg_method\n",
    ")\n",
    "fmri_conn_features = load_fmri_connectivity_features(\n",
    "    config.fmri_data_dir, config.overlap_subjects,\n",
    "    config.fmri_connectivity_types\n",
    ")\n",
    "fmri_label_dict = load_fmri_labels(config.fmri_label_path, config.overlap_subjects)\n",
    "\n",
    "logger.info(f'fMRI data loaded: Act={len(fmri_act_features)}, Conn={len(fmri_conn_features)}')\n",
    "print('Cell 4 complete: fMRI data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Subject Alignment & Bridge Dataset\n",
    "\n",
    "# Use EEG labels as ground truth (both pipelines share same subjects 1-32)\n",
    "# Prefer EEG labels; fall back to fMRI if an EEG label is missing\n",
    "bridge_labels = {}\n",
    "for subj in config.overlap_subjects:\n",
    "    if subj in eeg_label_dict:\n",
    "        bridge_labels[subj] = eeg_label_dict[subj]\n",
    "    elif subj in fmri_label_dict:\n",
    "        bridge_labels[subj] = fmri_label_dict[subj]\n",
    "\n",
    "logger.info(f'Bridge labels: {len(bridge_labels)} subjects')\n",
    "logger.info(f'Class distribution: {dict(zip(*np.unique(list(bridge_labels.values()), return_counts=True)))}')\n",
    "\n",
    "\n",
    "class BridgeRawDataset(Dataset):\n",
    "    \"\"\"Dataset that holds aligned raw EEG + fMRI data per subject.\n",
    "\n",
    "    EEG has multiple samples per subject (band x freq combos).\n",
    "    For each subject we store the raw fMRI tensors and a list of\n",
    "    EEG (erp, pw, conn) tuples. During __getitem__ we return all\n",
    "    EEG samples for one subject so the feature extractor can aggregate.\n",
    "    \"\"\"\n",
    "    def __init__(self, eeg_erp, eeg_pw, eeg_conn, fmri_act, fmri_conn,\n",
    "                 labels, subject_list, bands, func_segments):\n",
    "        self.samples = []\n",
    "\n",
    "        # Build per-subject EEG sample lists\n",
    "        eeg_by_subj = defaultdict(list)\n",
    "        for key, erp_val in eeg_erp.items():\n",
    "            subj = key[0]\n",
    "            band = key[1]\n",
    "            freq = key[2]\n",
    "            label_val = key[3]\n",
    "            pw_val = eeg_pw.get(key)\n",
    "            # Look up connectivity\n",
    "            lookup_band = str(band).lower()\n",
    "            conn_val = None\n",
    "            for cond in func_segments:\n",
    "                conn_key = (subj, lookup_band, cond, label_val)\n",
    "                if conn_key in eeg_conn:\n",
    "                    conn_val = eeg_conn[conn_key]\n",
    "                    break\n",
    "            if pw_val is not None and conn_val is not None:\n",
    "                eeg_by_subj[subj].append((erp_val, pw_val, conn_val))\n",
    "\n",
    "        # Align subjects\n",
    "        for subj in sorted(subject_list):\n",
    "            if (subj not in eeg_by_subj or subj not in fmri_act or\n",
    "                    subj not in fmri_conn or subj not in labels):\n",
    "                continue\n",
    "            self.samples.append({\n",
    "                'subject': subj,\n",
    "                'label': labels[subj],\n",
    "                'eeg_samples': eeg_by_subj[subj],\n",
    "                'fmri_act': fmri_act[subj],\n",
    "                'fmri_conn': fmri_conn[subj],\n",
    "            })\n",
    "\n",
    "        logger.info(f'BridgeRawDataset: {len(self.samples)} aligned subjects')\n",
    "        logger.info(f'  EEG samples per subject: '\n",
    "                    f'min={min(len(s[\"eeg_samples\"]) for s in self.samples)}, '\n",
    "                    f'max={max(len(s[\"eeg_samples\"]) for s in self.samples)}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        return s['eeg_samples'], s['fmri_act'], s['fmri_conn'], s['label'], s['subject']\n",
    "\n",
    "\n",
    "bridge_raw_dataset = BridgeRawDataset(\n",
    "    eeg_erp_features, eeg_pw_features, eeg_conn_features,\n",
    "    fmri_act_features, fmri_conn_features,\n",
    "    bridge_labels, config.overlap_subjects,\n",
    "    config.bands, config.func_segments\n",
    ")\n",
    "\n",
    "print(f'\\nAligned subjects: {len(bridge_raw_dataset)}')\n",
    "print('Cell 5 complete: Subject alignment done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Load Pre-trained Models\n",
    "\n",
    "def find_best_checkpoint(checkpoint_dir, pattern):\n",
    "    \"\"\"Find the best checkpoint file matching a glob pattern.\"\"\"\n",
    "    files = sorted(glob.glob(str(Path(checkpoint_dir) / pattern)))\n",
    "    if not files:\n",
    "        logger.warning(f'No checkpoint found matching {checkpoint_dir}/{pattern}')\n",
    "        return None\n",
    "    # Return the last one (most recent fold or best)\n",
    "    return files[-1]\n",
    "\n",
    "\n",
    "def load_eeg_model(checkpoint_path, dataset_sample, fusion_dim=128):\n",
    "    \"\"\"Instantiate and load an EEG tri-modal model from checkpoint.\"\"\"\n",
    "    eeg_samples = dataset_sample[0]  # list of (erp, pw, conn)\n",
    "    sample_erp, sample_pw, sample_conn = eeg_samples[0]\n",
    "\n",
    "    # Determine input dimensions\n",
    "    if sample_erp.ndim == 2:\n",
    "        in_erp_dim = sample_erp.shape[0]  # channels\n",
    "    else:\n",
    "        in_erp_dim = sample_erp.shape[0]\n",
    "\n",
    "    if sample_pw.ndim == 2:\n",
    "        in_pw_dim = sample_pw.shape[0]\n",
    "    else:\n",
    "        in_pw_dim = sample_pw.shape[0]\n",
    "\n",
    "    in_conn_dim = sample_conn.shape[0] if sample_conn.ndim == 1 else np.prod(sample_conn.shape)\n",
    "\n",
    "    logger.info(f'EEG model dims: ERP={in_erp_dim}, PW={in_pw_dim}, CONN={in_conn_dim}')\n",
    "\n",
    "    model = ImprovedTriModalFusionNet(\n",
    "        in_pw_dim=in_pw_dim,\n",
    "        in_erp_dim=in_erp_dim,\n",
    "        in_conn_dim=in_conn_dim,\n",
    "        fusion_dim=fusion_dim,\n",
    "        num_classes=config.num_classes\n",
    "    )\n",
    "\n",
    "    if checkpoint_path:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        logger.info(f'EEG model loaded from {checkpoint_path}')\n",
    "    else:\n",
    "        logger.warning('No EEG checkpoint found, using random weights')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    model.requires_grad_(False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_fmri_model(checkpoint_path, fmri_act_dim, fmri_conn_dim, hidden_dim=64):\n",
    "    \"\"\"Instantiate and load an fMRI fusion model from checkpoint.\"\"\"\n",
    "    model = fMRIFusionNet(\n",
    "        activation_dim=fmri_act_dim,\n",
    "        connectivity_dim=fmri_conn_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_classes=config.num_classes\n",
    "    )\n",
    "\n",
    "    if checkpoint_path:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        logger.info(f'fMRI model loaded from {checkpoint_path}')\n",
    "    else:\n",
    "        logger.warning('No fMRI checkpoint found, using random weights')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    model.requires_grad_(False)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Find checkpoints\n",
    "eeg_ckpt = find_best_checkpoint(config.eeg_checkpoint_dir, 'best_trimodal_fold*.pt')\n",
    "fmri_ckpt = find_best_checkpoint(config.fmri_checkpoint_dir, 'best_fusion_fold*.pt')\n",
    "\n",
    "# Determine fMRI input dimensions\n",
    "sample_fmri_act = list(fmri_act_features.values())[0]\n",
    "sample_fmri_conn = list(fmri_conn_features.values())[0]\n",
    "fmri_act_dim = sample_fmri_act.shape[0]\n",
    "fmri_conn_dim = sample_fmri_conn.shape[0]\n",
    "logger.info(f'fMRI dims: activation={fmri_act_dim}, connectivity={fmri_conn_dim}')\n",
    "\n",
    "# Load models\n",
    "sample_data = bridge_raw_dataset[0]\n",
    "eeg_model = load_eeg_model(eeg_ckpt, sample_data, fusion_dim=config.eeg_hidden_dim)\n",
    "fmri_model = load_fmri_model(fmri_ckpt, fmri_act_dim, fmri_conn_dim, hidden_dim=config.fmri_hidden_dim)\n",
    "\n",
    "n_eeg_params = sum(p.numel() for p in eeg_model.parameters())\n",
    "n_fmri_params = sum(p.numel() for p in fmri_model.parameters())\n",
    "logger.info(f'EEG model params: {n_eeg_params:,} (all frozen)')\n",
    "logger.info(f'fMRI model params: {n_fmri_params:,} (all frozen)')\n",
    "\n",
    "print('Cell 6 complete: Pre-trained models loaded and frozen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Feature Extraction Functions\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_eeg_features(model, raw_dataset, device):\n",
    "    \"\"\"Extract fused features from frozen EEG model.\n",
    "\n",
    "    EEG has multiple samples per subject (band x freq). We mean-pool\n",
    "    the fused features across all samples to get one 128-d vector per subject.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    features = {}\n",
    "\n",
    "    for idx in range(len(raw_dataset)):\n",
    "        eeg_samples, _, _, label, subj = raw_dataset[idx]\n",
    "        feat_list = []\n",
    "\n",
    "        for erp_np, pw_np, conn_np in eeg_samples:\n",
    "            erp_t = torch.tensor(erp_np, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            pw_t = torch.tensor(pw_np, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            conn_t = torch.tensor(conn_np, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            # Flatten conn if needed\n",
    "            if conn_t.dim() > 2:\n",
    "                conn_t = conn_t.view(conn_t.size(0), -1)\n",
    "\n",
    "            try:\n",
    "                out = model(erp=erp_t, pw=pw_t, conn=conn_t, return_feats=True)\n",
    "                fused = out['fused_feats']  # (1, 128)\n",
    "                feat_list.append(fused.cpu())\n",
    "            except Exception as e:\n",
    "                # Skip samples that cause dimension mismatches\n",
    "                continue\n",
    "\n",
    "        if feat_list:\n",
    "            # Mean pool across all band x freq samples\n",
    "            stacked = torch.cat(feat_list, dim=0)  # (N, 128)\n",
    "            mean_feat = stacked.mean(dim=0)  # (128,)\n",
    "            features[subj] = mean_feat\n",
    "\n",
    "    logger.info(f'Extracted EEG features for {len(features)} subjects, dim={list(features.values())[0].shape[0] if features else \"N/A\"}')\n",
    "    return features\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_fmri_features(model, fmri_act, fmri_conn, subject_list, device):\n",
    "    \"\"\"Extract fused features from frozen fMRI model.\"\"\"\n",
    "    model.eval()\n",
    "    features = {}\n",
    "\n",
    "    for subj in subject_list:\n",
    "        if subj not in fmri_act or subj not in fmri_conn:\n",
    "            continue\n",
    "        act_t = fmri_act[subj].unsqueeze(0).to(device)\n",
    "        conn_t = fmri_conn[subj].unsqueeze(0).to(device)\n",
    "\n",
    "        try:\n",
    "            _, fused = model(act_t, conn_t, return_features=True)\n",
    "            features[subj] = fused.squeeze(0).cpu()  # (64,)\n",
    "        except Exception as e:\n",
    "            logger.warning(f'fMRI feature extraction failed for subject {subj}: {e}')\n",
    "\n",
    "    logger.info(f'Extracted fMRI features for {len(features)} subjects, dim={list(features.values())[0].shape[0] if features else \"N/A\"}')\n",
    "    return features\n",
    "\n",
    "\n",
    "# Extract features\n",
    "logger.info('Extracting EEG fused features...')\n",
    "eeg_fused_features = extract_eeg_features(eeg_model, bridge_raw_dataset, device)\n",
    "\n",
    "logger.info('Extracting fMRI fused features...')\n",
    "fmri_fused_features = extract_fmri_features(\n",
    "    fmri_model, fmri_act_features, fmri_conn_features, config.overlap_subjects, device\n",
    ")\n",
    "\n",
    "# Verify alignment\n",
    "common_subjects = sorted(set(eeg_fused_features.keys()) & set(fmri_fused_features.keys()) & set(bridge_labels.keys()))\n",
    "logger.info(f'Common subjects with both EEG and fMRI features: {len(common_subjects)}')\n",
    "\n",
    "if common_subjects:\n",
    "    s = common_subjects[0]\n",
    "    logger.info(f'  Sample subject {s}: EEG={eeg_fused_features[s].shape}, fMRI={fmri_fused_features[s].shape}')\n",
    "\n",
    "print('Cell 7 complete: Features extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Bridge Fusion Model\n",
    "\n",
    "class EEGfMRIBridgeFusionNet(nn.Module):\n",
    "    \"\"\"Cross-modality bridge fusion: EEG (128-d) + fMRI (64-d).\n",
    "\n",
    "    Projects both modalities to a shared space, applies cross-modal\n",
    "    attention, learned temperature-scaled fusion, and classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, eeg_dim=128, fmri_dim=64, bridge_dim=128,\n",
    "                 num_classes=2, num_heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bridge_dim = bridge_dim\n",
    "\n",
    "        # Project to shared space\n",
    "        self.eeg_proj = nn.Sequential(\n",
    "            nn.Linear(eeg_dim, bridge_dim),\n",
    "            nn.LayerNorm(bridge_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.fmri_proj = nn.Sequential(\n",
    "            nn.Linear(fmri_dim, bridge_dim),\n",
    "            nn.LayerNorm(bridge_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Cross-modal attention (EEG and fMRI attend to each other)\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            bridge_dim, num_heads=num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Learned fusion with temperature scaling\n",
    "        self.fusion = LearnedFusionModule(\n",
    "            num_modalities=2,\n",
    "            hidden_dim=bridge_dim,\n",
    "            use_temperature=True\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bridge_dim, bridge_dim // 2),\n",
    "            nn.BatchNorm1d(bridge_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(bridge_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, eeg_feats, fmri_feats, return_features=False, return_weights=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            eeg_feats: (batch, eeg_dim)\n",
    "            fmri_feats: (batch, fmri_dim)\n",
    "        Returns:\n",
    "            logits, and optionally fused features and weights.\n",
    "        \"\"\"\n",
    "        # Project to shared space\n",
    "        eeg_proj = self.eeg_proj(eeg_feats)    # (batch, bridge_dim)\n",
    "        fmri_proj = self.fmri_proj(fmri_feats)  # (batch, bridge_dim)\n",
    "\n",
    "        # Cross-modal attention: stack as sequence of 2 tokens\n",
    "        modality_seq = torch.stack([eeg_proj, fmri_proj], dim=1)  # (batch, 2, bridge_dim)\n",
    "\n",
    "        # EEG attends to both modalities\n",
    "        eeg_q = eeg_proj.unsqueeze(1)  # (batch, 1, bridge_dim)\n",
    "        attn_out, attn_weights_raw = self.cross_attn(\n",
    "            eeg_q, modality_seq, modality_seq\n",
    "        )\n",
    "        eeg_enhanced = attn_out.squeeze(1)  # (batch, bridge_dim)\n",
    "\n",
    "        # Learned fusion\n",
    "        if return_weights:\n",
    "            fused, fusion_weights = self.fusion(\n",
    "                [eeg_enhanced, fmri_proj], return_weights=True\n",
    "            )\n",
    "        else:\n",
    "            fused = self.fusion([eeg_enhanced, fmri_proj])\n",
    "            fusion_weights = None\n",
    "\n",
    "        # Classify\n",
    "        logits = self.classifier(fused)\n",
    "\n",
    "        results = [logits]\n",
    "        if return_features:\n",
    "            results.append(fused)\n",
    "        if return_weights:\n",
    "            results.append(fusion_weights)\n",
    "            results.append(attn_weights_raw)\n",
    "\n",
    "        return results[0] if len(results) == 1 else tuple(results)\n",
    "\n",
    "    def get_fusion_weights(self):\n",
    "        with torch.no_grad():\n",
    "            logits = self.fusion.fusion_logits\n",
    "            temp = self.fusion.temperature\n",
    "            weights = F.softmax(logits / temp, dim=0)\n",
    "            return {\n",
    "                'eeg_weight': weights[0].item(),\n",
    "                'fmri_weight': weights[1].item(),\n",
    "                'temperature': temp.item()\n",
    "            }\n",
    "\n",
    "\n",
    "# Quick architecture test\n",
    "_test_bridge = EEGfMRIBridgeFusionNet(\n",
    "    eeg_dim=config.eeg_hidden_dim,\n",
    "    fmri_dim=config.fmri_hidden_dim,\n",
    "    bridge_dim=config.bridge_hidden_dim,\n",
    "    num_classes=config.num_classes,\n",
    "    dropout=config.dropout\n",
    ")\n",
    "n_bridge_params = sum(p.numel() for p in _test_bridge.parameters())\n",
    "n_trainable = sum(p.numel() for p in _test_bridge.parameters() if p.requires_grad)\n",
    "print(f'Bridge model: {n_bridge_params:,} total params, {n_trainable:,} trainable')\n",
    "\n",
    "# Smoke test\n",
    "_eeg_dummy = torch.randn(4, config.eeg_hidden_dim)\n",
    "_fmri_dummy = torch.randn(4, config.fmri_hidden_dim)\n",
    "_logits, _fused, _fw, _aw = _test_bridge(_eeg_dummy, _fmri_dummy, return_features=True, return_weights=True)\n",
    "print(f'Smoke test: logits={_logits.shape}, fused={_fused.shape}, fusion_weights={_fw.shape}, attn_weights={_aw.shape}')\n",
    "del _test_bridge, _eeg_dummy, _fmri_dummy\n",
    "\n",
    "print('Cell 8 complete: Bridge fusion model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Bridge Dataset from Pre-extracted Features\n",
    "\n",
    "class BridgeFeatureDataset(Dataset):\n",
    "    \"\"\"Dataset of pre-extracted EEG and fMRI features, aligned by subject.\"\"\"\n",
    "    def __init__(self, eeg_features, fmri_features, labels, subject_list):\n",
    "        self.samples = []\n",
    "        for subj in sorted(subject_list):\n",
    "            if subj in eeg_features and subj in fmri_features and subj in labels:\n",
    "                self.samples.append({\n",
    "                    'eeg': eeg_features[subj],\n",
    "                    'fmri': fmri_features[subj],\n",
    "                    'label': labels[subj],\n",
    "                    'subject': subj\n",
    "                })\n",
    "        logger.info(f'BridgeFeatureDataset: {len(self.samples)} samples')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        return s['eeg'], s['fmri'], s['label'], s['subject']\n",
    "\n",
    "\n",
    "def collate_bridge(batch):\n",
    "    eeg = torch.stack([b[0] for b in batch])\n",
    "    fmri = torch.stack([b[1] for b in batch])\n",
    "    labels = torch.tensor([b[2] for b in batch], dtype=torch.long)\n",
    "    subjects = [b[3] for b in batch]\n",
    "    return eeg, fmri, labels, subjects\n",
    "\n",
    "\n",
    "bridge_dataset = BridgeFeatureDataset(\n",
    "    eeg_fused_features, fmri_fused_features, bridge_labels, common_subjects\n",
    ")\n",
    "\n",
    "all_labels = np.array([s['label'] for s in bridge_dataset.samples])\n",
    "print(f'Bridge dataset: {len(bridge_dataset)} samples')\n",
    "print(f'Class distribution: {dict(zip(*np.unique(all_labels, return_counts=True)))}')\n",
    "print('Cell 9 complete: Bridge feature dataset created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Training Loop\n",
    "\n",
    "def train_bridge_epoch(model, loader, optimizer, criterion, device, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for eeg, fmri, labels, _ in loader:\n",
    "        eeg, fmri, labels = eeg.to(device), fmri.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(eeg, fmri)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        if grad_clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(len(loader), 1)\n",
    "\n",
    "\n",
    "def evaluate_bridge(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_targets, all_probs = [], [], []\n",
    "    all_subjects = []\n",
    "    with torch.no_grad():\n",
    "        for eeg, fmri, labels, subjects in loader:\n",
    "            eeg, fmri = eeg.to(device), fmri.to(device)\n",
    "            logits = model(eeg, fmri)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(labels.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_subjects.extend(subjects)\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(all_targets, all_preds),\n",
    "        'F1': f1_score(all_targets, all_preds, average='weighted', zero_division=0),\n",
    "        'Precision': precision_score(all_targets, all_preds, average='weighted', zero_division=0),\n",
    "        'Recall': recall_score(all_targets, all_preds, average='weighted', zero_division=0),\n",
    "    }\n",
    "    try:\n",
    "        metrics['AUC'] = roc_auc_score(all_targets, all_probs[:, 1])\n",
    "    except Exception:\n",
    "        metrics['AUC'] = 0.5\n",
    "    return metrics, all_targets, all_probs, all_subjects\n",
    "\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=SEED)\n",
    "fold_results = []\n",
    "fold_fusion_weights = []\n",
    "fold_roc_data = []\n",
    "all_fold_fused_features = {}  # subject -> fused features (for visualization)\n",
    "\n",
    "logger.info(f'Starting {config.n_splits}-fold cross-validation')\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(cv.split(np.zeros(len(bridge_dataset)), all_labels), 1):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'FOLD {fold_idx}/{config.n_splits}')\n",
    "    print(f'{\"=\"*60}')\n",
    "\n",
    "    train_subset = Subset(bridge_dataset, train_idx)\n",
    "    test_subset = Subset(bridge_dataset, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=config.batch_size,\n",
    "                              shuffle=True, collate_fn=collate_bridge)\n",
    "    test_loader = DataLoader(test_subset, batch_size=config.batch_size,\n",
    "                             shuffle=False, collate_fn=collate_bridge)\n",
    "\n",
    "    # Class weights\n",
    "    train_labels = all_labels[train_idx]\n",
    "    cw = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "    cw_tensor = torch.tensor(cw, dtype=torch.float32).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=cw_tensor)\n",
    "\n",
    "    # Create bridge model\n",
    "    bridge_model = EEGfMRIBridgeFusionNet(\n",
    "        eeg_dim=config.eeg_hidden_dim,\n",
    "        fmri_dim=config.fmri_hidden_dim,\n",
    "        bridge_dim=config.bridge_hidden_dim,\n",
    "        num_classes=config.num_classes,\n",
    "        dropout=config.dropout\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(bridge_model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        train_loss = train_bridge_epoch(bridge_model, train_loader, optimizer, criterion, device, config.grad_clip)\n",
    "        test_metrics, _, _, _ = evaluate_bridge(bridge_model, test_loader, device)\n",
    "        scheduler.step(1.0 - test_metrics['F1'])\n",
    "\n",
    "        if test_metrics['F1'] > best_f1:\n",
    "            best_f1 = test_metrics['F1']\n",
    "            best_state = copy.deepcopy(bridge_model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            logger.info(f'  Fold {fold_idx} Epoch {epoch}: loss={train_loss:.4f}, '\n",
    "                        f'F1={test_metrics[\"F1\"]:.4f}, Acc={test_metrics[\"Accuracy\"]:.4f}')\n",
    "\n",
    "        if patience_counter >= config.patience:\n",
    "            logger.info(f'  Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    if best_state:\n",
    "        bridge_model.load_state_dict(best_state)\n",
    "\n",
    "    # Final evaluation\n",
    "    test_metrics, targets, probs, subjects = evaluate_bridge(bridge_model, test_loader, device)\n",
    "    fold_results.append(test_metrics)\n",
    "\n",
    "    # Collect ROC data\n",
    "    fold_roc_data.append((targets, probs[:, 1]))\n",
    "\n",
    "    # Collect fusion weights\n",
    "    fw = bridge_model.get_fusion_weights()\n",
    "    fold_fusion_weights.append(fw)\n",
    "\n",
    "    # Extract fused features for visualization\n",
    "    bridge_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for eeg, fmri, labels, subjs in test_loader:\n",
    "            eeg, fmri = eeg.to(device), fmri.to(device)\n",
    "            _, fused, _, _ = bridge_model(eeg, fmri, return_features=True, return_weights=True)\n",
    "            for s, feat in zip(subjs, fused.cpu()):\n",
    "                all_fold_fused_features[s] = feat\n",
    "\n",
    "    # Save checkpoint\n",
    "    ckpt_path = config.checkpoint_dir / f'best_bridge_fold{fold_idx}.pt'\n",
    "    torch.save({\n",
    "        'model_state_dict': bridge_model.state_dict(),\n",
    "        'metrics': test_metrics,\n",
    "        'fold': fold_idx,\n",
    "        'fusion_weights': fw\n",
    "    }, ckpt_path)\n",
    "\n",
    "    logger.info(f'Fold {fold_idx} Results: Acc={test_metrics[\"Accuracy\"]:.4f}, '\n",
    "                f'F1={test_metrics[\"F1\"]:.4f}, AUC={test_metrics[\"AUC\"]:.4f}')\n",
    "    logger.info(f'  Fusion weights: EEG={fw[\"eeg_weight\"]:.3f}, fMRI={fw[\"fmri_weight\"]:.3f}, '\n",
    "                f'T={fw[\"temperature\"]:.3f}')\n",
    "\n",
    "# Aggregate results\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print('BRIDGE FUSION RESULTS SUMMARY')\n",
    "print(f'{\"=\"*60}')\n",
    "for metric in ['Accuracy', 'F1', 'Precision', 'Recall', 'AUC']:\n",
    "    values = [r[metric] for r in fold_results]\n",
    "    print(f'  {metric:12s}: {np.mean(values):.4f} +/- {np.std(values):.4f}')\n",
    "\n",
    "eeg_w = [fw['eeg_weight'] for fw in fold_fusion_weights]\n",
    "fmri_w = [fw['fmri_weight'] for fw in fold_fusion_weights]\n",
    "print(f'\\n  EEG weight:  {np.mean(eeg_w):.4f} +/- {np.std(eeg_w):.4f}')\n",
    "print(f'  fMRI weight: {np.mean(fmri_w):.4f} +/- {np.std(fmri_w):.4f}')\n",
    "\n",
    "print('\\nCell 10 complete: Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Results Visualization\n",
    "\n",
    "fig_dir = config.output_dir / 'figures'\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 1. Performance Summary Table ---\n",
    "summary_rows = []\n",
    "for metric in ['Accuracy', 'F1', 'Precision', 'Recall', 'AUC']:\n",
    "    values = [r[metric] for r in fold_results]\n",
    "    summary_rows.append({\n",
    "        'Metric': metric,\n",
    "        'Mean': np.mean(values),\n",
    "        'Std': np.std(values),\n",
    "        'Summary': f'{np.mean(values):.4f} +/- {np.std(values):.4f}'\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(config.output_dir / f'bridge_summary_{timestamp}.csv', index=False)\n",
    "print('Performance Summary:')\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# --- 2. ROC Curves ---\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for fold_idx, (targets, scores) in enumerate(fold_roc_data, 1):\n",
    "    fpr, tpr, _ = roc_curve(targets, scores)\n",
    "    auc_val = roc_auc_score(targets, scores) if len(np.unique(targets)) > 1 else 0.5\n",
    "    ax.plot(fpr, tpr, label=f'Fold {fold_idx} (AUC={auc_val:.3f})')\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Bridge Fusion ROC Curves')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / f'roc_curves_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- 3. Confusion Matrices ---\n",
    "fig, axes = plt.subplots(1, config.n_splits, figsize=(4 * config.n_splits, 4))\n",
    "if config.n_splits == 1:\n",
    "    axes = [axes]\n",
    "for fold_idx, (targets, scores) in enumerate(fold_roc_data):\n",
    "    preds = (scores >= 0.5).astype(int)\n",
    "    cm = confusion_matrix(targets, preds)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[fold_idx],\n",
    "                xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "    axes[fold_idx].set_title(f'Fold {fold_idx + 1}')\n",
    "    axes[fold_idx].set_ylabel('True')\n",
    "    axes[fold_idx].set_xlabel('Predicted')\n",
    "plt.suptitle('Confusion Matrices', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / f'confusion_matrices_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- 4. Fusion Weight Evolution ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "folds = list(range(1, config.n_splits + 1))\n",
    "eeg_weights = [fw['eeg_weight'] for fw in fold_fusion_weights]\n",
    "fmri_weights = [fw['fmri_weight'] for fw in fold_fusion_weights]\n",
    "\n",
    "axes[0].plot(folds, eeg_weights, 'o-', label='EEG', color='#2ecc71', linewidth=2, markersize=8)\n",
    "axes[0].plot(folds, fmri_weights, 's-', label='fMRI', color='#e74c3c', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Fold')\n",
    "axes[0].set_ylabel('Weight')\n",
    "axes[0].set_title('Fusion Weights Across Folds')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "bars = axes[1].bar(['EEG', 'fMRI'],\n",
    "                    [np.mean(eeg_weights), np.mean(fmri_weights)],\n",
    "                    yerr=[np.std(eeg_weights), np.std(fmri_weights)],\n",
    "                    capsize=10, color=['#2ecc71', '#e74c3c'], edgecolor='black', alpha=0.8)\n",
    "axes[1].set_ylabel('Average Weight')\n",
    "axes[1].set_title('Average Fusion Weights')\n",
    "axes[1].set_ylim(0, 1)\n",
    "for bar, mean in zip(bars, [np.mean(eeg_weights), np.mean(fmri_weights)]):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                 f'{mean:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / f'fusion_weights_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- 5. t-SNE of Bridge Fused Features ---\n",
    "if all_fold_fused_features:\n",
    "    from sklearn.manifold import TSNE\n",
    "    feat_subjects = sorted(all_fold_fused_features.keys())\n",
    "    feat_matrix = np.stack([all_fold_fused_features[s].numpy() for s in feat_subjects])\n",
    "    feat_labels = np.array([bridge_labels[s] for s in feat_subjects])\n",
    "\n",
    "    if len(feat_subjects) > 5:\n",
    "        perplexity = min(30, len(feat_subjects) - 1)\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=SEED)\n",
    "        embedded = tsne.fit_transform(feat_matrix)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        for cls in np.unique(feat_labels):\n",
    "            mask = feat_labels == cls\n",
    "            ax.scatter(embedded[mask, 0], embedded[mask, 1],\n",
    "                       label=f'Class {cls}', alpha=0.7, s=60)\n",
    "        ax.set_title('t-SNE of Bridge Fused Features')\n",
    "        ax.set_xlabel('t-SNE 1')\n",
    "        ax.set_ylabel('t-SNE 2')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_dir / f'tsne_bridge_features_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "print('Cell 11 complete: Visualizations saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: XAI - Gradient Saliency\n",
    "\n",
    "class BridgeGradientSaliency:\n",
    "    \"\"\"Gradient saliency for the bridge model.\"\"\"\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def compute(self, eeg_feats, fmri_feats, target_class=None):\n",
    "        self.model.eval()\n",
    "        eeg_feats = eeg_feats.clone().detach().to(self.device).requires_grad_(True)\n",
    "        fmri_feats = fmri_feats.clone().detach().to(self.device).requires_grad_(True)\n",
    "\n",
    "        logits = self.model(eeg_feats, fmri_feats)\n",
    "\n",
    "        if target_class is None:\n",
    "            target_class = logits.argmax(dim=1)\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        one_hot = torch.zeros_like(logits)\n",
    "        one_hot.scatter_(1, target_class.view(-1, 1), 1)\n",
    "        logits.backward(gradient=one_hot)\n",
    "\n",
    "        return {\n",
    "            'eeg': eeg_feats.grad.abs().cpu().numpy(),\n",
    "            'fmri': fmri_feats.grad.abs().cpu().numpy()\n",
    "        }\n",
    "\n",
    "\n",
    "# Load the best fold model for XAI\n",
    "best_fold_idx = np.argmax([r['F1'] for r in fold_results]) + 1\n",
    "xai_model = EEGfMRIBridgeFusionNet(\n",
    "    eeg_dim=config.eeg_hidden_dim,\n",
    "    fmri_dim=config.fmri_hidden_dim,\n",
    "    bridge_dim=config.bridge_hidden_dim,\n",
    "    num_classes=config.num_classes,\n",
    "    dropout=config.dropout\n",
    ").to(device)\n",
    "\n",
    "xai_ckpt = torch.load(config.checkpoint_dir / f'best_bridge_fold{best_fold_idx}.pt',\n",
    "                       map_location=device, weights_only=False)\n",
    "xai_model.load_state_dict(xai_ckpt['model_state_dict'])\n",
    "logger.info(f'Loaded best model from fold {best_fold_idx} for XAI')\n",
    "\n",
    "# Compute gradient saliency for all subjects\n",
    "saliency = BridgeGradientSaliency(xai_model, device)\n",
    "all_eeg_saliency = []\n",
    "all_fmri_saliency = []\n",
    "\n",
    "for idx in range(len(bridge_dataset)):\n",
    "    eeg, fmri, label, subj = bridge_dataset[idx]\n",
    "    result = saliency.compute(eeg.unsqueeze(0), fmri.unsqueeze(0))\n",
    "    all_eeg_saliency.append(result['eeg'].squeeze())\n",
    "    all_fmri_saliency.append(result['fmri'].squeeze())\n",
    "\n",
    "mean_eeg_sal = np.mean(all_eeg_saliency, axis=0)\n",
    "mean_fmri_sal = np.mean(all_fmri_saliency, axis=0)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(range(len(mean_eeg_sal)), mean_eeg_sal, color='#2ecc71', alpha=0.7)\n",
    "axes[0].set_title('Gradient Saliency: EEG Features')\n",
    "axes[0].set_xlabel('Feature Dimension')\n",
    "axes[0].set_ylabel('Saliency')\n",
    "\n",
    "axes[1].bar(range(len(mean_fmri_sal)), mean_fmri_sal, color='#e74c3c', alpha=0.7)\n",
    "axes[1].set_title('Gradient Saliency: fMRI Features')\n",
    "axes[1].set_xlabel('Feature Dimension')\n",
    "axes[1].set_ylabel('Saliency')\n",
    "\n",
    "plt.suptitle('Gradient Saliency Analysis', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / f'gradient_saliency_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Modality importance comparison\n",
    "total_eeg = np.sum(mean_eeg_sal)\n",
    "total_fmri = np.sum(mean_fmri_sal)\n",
    "total = total_eeg + total_fmri\n",
    "print(f'Gradient saliency modality importance:')\n",
    "print(f'  EEG:  {total_eeg/total:.4f} ({total_eeg:.4f})')\n",
    "print(f'  fMRI: {total_fmri/total:.4f} ({total_fmri:.4f})')\n",
    "\n",
    "print('Cell 12 complete: Gradient saliency analysis done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: XAI - Integrated Gradients\n",
    "\n",
    "class BridgeIntegratedGradients:\n",
    "    \"\"\"Integrated Gradients for the bridge model.\"\"\"\n",
    "    def __init__(self, model, device, n_steps=50):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "    def compute(self, eeg_feats, fmri_feats, target_class=None):\n",
    "        self.model.eval()\n",
    "        eeg_feats = eeg_feats.to(self.device)\n",
    "        fmri_feats = fmri_feats.to(self.device)\n",
    "\n",
    "        eeg_baseline = torch.zeros_like(eeg_feats)\n",
    "        fmri_baseline = torch.zeros_like(fmri_feats)\n",
    "\n",
    "        eeg_diff = eeg_feats - eeg_baseline\n",
    "        fmri_diff = fmri_feats - fmri_baseline\n",
    "\n",
    "        eeg_grads, fmri_grads = [], []\n",
    "\n",
    "        for alpha in np.linspace(0, 1, self.n_steps):\n",
    "            eeg_interp = (eeg_baseline + alpha * eeg_diff).requires_grad_(True)\n",
    "            fmri_interp = (fmri_baseline + alpha * fmri_diff).requires_grad_(True)\n",
    "\n",
    "            logits = self.model(eeg_interp, fmri_interp)\n",
    "\n",
    "            if target_class is None:\n",
    "                target_class = logits.argmax(dim=1)\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            one_hot = torch.zeros_like(logits)\n",
    "            one_hot.scatter_(1, target_class.view(-1, 1), 1)\n",
    "            logits.backward(gradient=one_hot)\n",
    "\n",
    "            eeg_grads.append(eeg_interp.grad.detach().cpu().numpy())\n",
    "            fmri_grads.append(fmri_interp.grad.detach().cpu().numpy())\n",
    "\n",
    "        eeg_ig = eeg_diff.cpu().numpy() * np.mean(eeg_grads, axis=0)\n",
    "        fmri_ig = fmri_diff.cpu().numpy() * np.mean(fmri_grads, axis=0)\n",
    "\n",
    "        return {'eeg': np.abs(eeg_ig), 'fmri': np.abs(fmri_ig)}\n",
    "\n",
    "\n",
    "ig = BridgeIntegratedGradients(xai_model, device, n_steps=50)\n",
    "\n",
    "all_eeg_ig = []\n",
    "all_fmri_ig = []\n",
    "\n",
    "for idx in range(len(bridge_dataset)):\n",
    "    eeg, fmri, label, subj = bridge_dataset[idx]\n",
    "    result = ig.compute(eeg.unsqueeze(0), fmri.unsqueeze(0))\n",
    "    all_eeg_ig.append(result['eeg'].squeeze())\n",
    "    all_fmri_ig.append(result['fmri'].squeeze())\n",
    "\n",
    "mean_eeg_ig = np.mean(all_eeg_ig, axis=0)\n",
    "mean_fmri_ig = np.mean(all_fmri_ig, axis=0)\n",
    "\n",
    "# Plot top features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "top_k = 20\n",
    "eeg_top_idx = np.argsort(mean_eeg_ig)[-top_k:][::-1]\n",
    "fmri_top_idx = np.argsort(mean_fmri_ig)[-top_k:][::-1]\n",
    "\n",
    "axes[0].barh(range(top_k), mean_eeg_ig[eeg_top_idx], color='#2ecc71', alpha=0.7)\n",
    "axes[0].set_yticks(range(top_k))\n",
    "axes[0].set_yticklabels([f'EEG-{i}' for i in eeg_top_idx])\n",
    "axes[0].set_title(f'Top {top_k} EEG Feature Attributions (IG)')\n",
    "axes[0].set_xlabel('Attribution')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "axes[1].barh(range(top_k), mean_fmri_ig[fmri_top_idx], color='#e74c3c', alpha=0.7)\n",
    "axes[1].set_yticks(range(top_k))\n",
    "axes[1].set_yticklabels([f'fMRI-{i}' for i in fmri_top_idx])\n",
    "axes[1].set_title(f'Top {top_k} fMRI Feature Attributions (IG)')\n",
    "axes[1].set_xlabel('Attribution')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.suptitle('Integrated Gradients Attribution', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / f'integrated_gradients_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Modality comparison\n",
    "total_eeg_ig = np.sum(mean_eeg_ig)\n",
    "total_fmri_ig = np.sum(mean_fmri_ig)\n",
    "total_ig = total_eeg_ig + total_fmri_ig\n",
    "print(f'Integrated Gradients modality importance:')\n",
    "print(f'  EEG:  {total_eeg_ig/total_ig:.4f}')\n",
    "print(f'  fMRI: {total_fmri_ig/total_ig:.4f}')\n",
    "\n",
    "print('Cell 13 complete: Integrated gradients analysis done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: XAI - SHAP Analysis\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print('SHAP not available. Install with: pip install shap')\n",
    "\n",
    "if SHAP_AVAILABLE:\n",
    "    # Build wrapper function for SHAP\n",
    "    def bridge_predict(inputs):\n",
    "        \"\"\"Wrapper: concatenated [eeg, fmri] -> class probabilities.\"\"\"\n",
    "        inputs_t = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "        eeg_part = inputs_t[:, :config.eeg_hidden_dim]\n",
    "        fmri_part = inputs_t[:, config.eeg_hidden_dim:]\n",
    "        xai_model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = xai_model(eeg_part, fmri_part)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "    # Prepare data\n",
    "    all_features = []\n",
    "    for idx in range(len(bridge_dataset)):\n",
    "        eeg, fmri, _, _ = bridge_dataset[idx]\n",
    "        combined = torch.cat([eeg, fmri]).numpy()\n",
    "        all_features.append(combined)\n",
    "    all_features = np.array(all_features)\n",
    "\n",
    "    # Use a subset as background\n",
    "    n_background = min(20, len(all_features))\n",
    "    background = all_features[:n_background]\n",
    "\n",
    "    # Create SHAP explainer\n",
    "    explainer = shap.KernelExplainer(bridge_predict, background)\n",
    "    shap_values = explainer.shap_values(all_features, nsamples=100)\n",
    "\n",
    "    # SHAP values for positive class\n",
    "    if isinstance(shap_values, list):\n",
    "        sv = shap_values[1]  # Class 1\n",
    "    else:\n",
    "        sv = shap_values\n",
    "\n",
    "    # Split into EEG and fMRI\n",
    "    eeg_shap = sv[:, :config.eeg_hidden_dim]\n",
    "    fmri_shap = sv[:, config.eeg_hidden_dim:]\n",
    "\n",
    "    # Feature names\n",
    "    feature_names = ([f'EEG-{i}' for i in range(config.eeg_hidden_dim)] +\n",
    "                     [f'fMRI-{i}' for i in range(config.fmri_hidden_dim)])\n",
    "\n",
    "    # SHAP summary plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    shap.summary_plot(sv, all_features, feature_names=feature_names,\n",
    "                      max_display=20, show=False)\n",
    "    plt.title('SHAP Feature Importance (Top 20)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_dir / f'shap_summary_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Modality-level SHAP importance\n",
    "    eeg_importance = np.mean(np.abs(eeg_shap))\n",
    "    fmri_importance = np.mean(np.abs(fmri_shap))\n",
    "    total_shap = eeg_importance + fmri_importance\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    bars = ax.bar(['EEG', 'fMRI'],\n",
    "                  [eeg_importance/total_shap, fmri_importance/total_shap],\n",
    "                  color=['#2ecc71', '#e74c3c'], edgecolor='black', alpha=0.8)\n",
    "    ax.set_ylabel('Relative SHAP Importance')\n",
    "    ax.set_title('SHAP Modality Importance')\n",
    "    for bar in bars:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{bar.get_height():.3f}', ha='center', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_dir / f'shap_modality_importance_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'SHAP modality importance: EEG={eeg_importance/total_shap:.4f}, fMRI={fmri_importance/total_shap:.4f}')\n",
    "\n",
    "    # SHAP force plot for first subject\n",
    "    print('\\nSHAP force plot for subject', bridge_dataset.samples[0]['subject'], ':')\n",
    "    shap.force_plot(explainer.expected_value[1], sv[0], all_features[0],\n",
    "                    feature_names=feature_names, matplotlib=True, show=False)\n",
    "    plt.savefig(fig_dir / f'shap_force_subject1_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print('Cell 14 complete: SHAP analysis done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: XAI - Attention & Fusion Weight Analysis\n",
    "\n",
    "def extract_attention_and_fusion_weights(model, dataset, device):\n",
    "    \"\"\"Extract cross-modal attention weights and fusion weights per subject.\"\"\"\n",
    "    model.eval()\n",
    "    subject_data = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(dataset)):\n",
    "            eeg, fmri, label, subj = dataset[idx]\n",
    "            eeg_t = eeg.unsqueeze(0).to(device)\n",
    "            fmri_t = fmri.unsqueeze(0).to(device)\n",
    "\n",
    "            logits, fused, fusion_weights, attn_weights = model(\n",
    "                eeg_t, fmri_t, return_features=True, return_weights=True\n",
    "            )\n",
    "\n",
    "            subject_data.append({\n",
    "                'subject': subj,\n",
    "                'label': label,\n",
    "                'prediction': logits.argmax(dim=1).item(),\n",
    "                'fusion_weights': fusion_weights.cpu().numpy().squeeze(),\n",
    "                'attn_weights': attn_weights.cpu().numpy().squeeze(),\n",
    "            })\n",
    "\n",
    "    return subject_data\n",
    "\n",
    "\n",
    "subject_xai = extract_attention_and_fusion_weights(xai_model, bridge_dataset, device)\n",
    "\n",
    "# --- Attention Heatmap ---\n",
    "attn_matrix = np.stack([s['attn_weights'] for s in subject_xai])  # (N, num_heads, 1, 2) or (N, 1, 2)\n",
    "# Average across subjects and heads\n",
    "if attn_matrix.ndim == 4:\n",
    "    mean_attn = np.mean(attn_matrix, axis=(0, 1))  # (1, 2)\n",
    "elif attn_matrix.ndim == 3:\n",
    "    mean_attn = np.mean(attn_matrix, axis=0)  # (1, 2)\n",
    "else:\n",
    "    mean_attn = attn_matrix.reshape(-1, 2).mean(axis=0, keepdims=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "sns.heatmap(mean_attn.reshape(1, -1), annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "            xticklabels=['EEG', 'fMRI'], yticklabels=['Query'], ax=ax)\n",
    "ax.set_title('Cross-Modal Attention Weights (Mean)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / f'attention_heatmap_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- Per-Subject Fusion Weights ---\n",
    "fusion_weights_arr = np.stack([s['fusion_weights'] for s in subject_xai])\n",
    "# fusion_weights_arr shape: (N, 2)\n",
    "if fusion_weights_arr.ndim == 1:\n",
    "    fusion_weights_arr = fusion_weights_arr.reshape(-1, 2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "subjects_list = [s['subject'] for s in subject_xai]\n",
    "x = np.arange(len(subjects_list))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, fusion_weights_arr[:, 0], width, label='EEG', color='#2ecc71', alpha=0.8)\n",
    "ax.bar(x + width/2, fusion_weights_arr[:, 1], width, label='fMRI', color='#e74c3c', alpha=0.8)\n",
    "ax.set_xlabel('Subject')\n",
    "ax.set_ylabel('Fusion Weight')\n",
    "ax.set_title('Per-Subject Dynamic Fusion Weights')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(subjects_list, rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / f'per_subject_fusion_weights_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- Class-wise Fusion Weight Comparison ---\n",
    "class0_mask = np.array([s['label'] == 0 for s in subject_xai])\n",
    "class1_mask = np.array([s['label'] == 1 for s in subject_xai])\n",
    "\n",
    "print('Fusion weights by class:')\n",
    "if class0_mask.any():\n",
    "    c0_eeg = fusion_weights_arr[class0_mask, 0].mean()\n",
    "    c0_fmri = fusion_weights_arr[class0_mask, 1].mean()\n",
    "    print(f'  Class 0: EEG={c0_eeg:.4f}, fMRI={c0_fmri:.4f}')\n",
    "if class1_mask.any():\n",
    "    c1_eeg = fusion_weights_arr[class1_mask, 0].mean()\n",
    "    c1_fmri = fusion_weights_arr[class1_mask, 1].mean()\n",
    "    print(f'  Class 1: EEG={c1_eeg:.4f}, fMRI={c1_fmri:.4f}')\n",
    "\n",
    "print('Cell 15 complete: Attention & fusion weight analysis done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Summary & Export\n",
    "\n",
    "print('=' * 70)\n",
    "print('EEG-fMRI BRIDGE FUSION - FINAL SUMMARY')\n",
    "print('=' * 70)\n",
    "\n",
    "# Performance table\n",
    "print(f'\\nDataset: {len(bridge_dataset)} subjects (overlap of EEG & fMRI)')\n",
    "print(f'Cross-validation: {config.n_splits}-fold stratified')\n",
    "print(f'\\nBridge Fusion Performance:')\n",
    "for metric in ['Accuracy', 'F1', 'Precision', 'Recall', 'AUC']:\n",
    "    values = [r[metric] for r in fold_results]\n",
    "    print(f'  {metric:12s}: {np.mean(values):.4f} +/- {np.std(values):.4f}')\n",
    "\n",
    "print(f'\\nLearned Fusion Weights:')\n",
    "print(f'  EEG:  {np.mean(eeg_w):.4f} +/- {np.std(eeg_w):.4f}')\n",
    "print(f'  fMRI: {np.mean(fmri_w):.4f} +/- {np.std(fmri_w):.4f}')\n",
    "\n",
    "# Save all results\n",
    "results_dict = {\n",
    "    'summary': summary_df.to_dict(),\n",
    "    'fold_results': fold_results,\n",
    "    'fold_fusion_weights': fold_fusion_weights,\n",
    "    'gradient_saliency': {\n",
    "        'eeg_mean': mean_eeg_sal.tolist(),\n",
    "        'fmri_mean': mean_fmri_sal.tolist()\n",
    "    },\n",
    "    'integrated_gradients': {\n",
    "        'eeg_mean': mean_eeg_ig.tolist(),\n",
    "        'fmri_mean': mean_fmri_ig.tolist()\n",
    "    },\n",
    "    'per_subject_fusion_weights': [\n",
    "        {'subject': s['subject'], 'label': s['label'],\n",
    "         'eeg_weight': float(s['fusion_weights'][0]),\n",
    "         'fmri_weight': float(s['fusion_weights'][1])}\n",
    "        for s in subject_xai\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save as CSV\n",
    "fold_df = pd.DataFrame(fold_results)\n",
    "fold_df['Fold'] = range(1, config.n_splits + 1)\n",
    "fold_df.to_csv(config.output_dir / f'bridge_fold_results_{timestamp}.csv', index=False)\n",
    "\n",
    "fw_df = pd.DataFrame(fold_fusion_weights)\n",
    "fw_df['Fold'] = range(1, config.n_splits + 1)\n",
    "fw_df.to_csv(config.output_dir / f'bridge_fusion_weights_{timestamp}.csv', index=False)\n",
    "\n",
    "subj_fw_df = pd.DataFrame(results_dict['per_subject_fusion_weights'])\n",
    "subj_fw_df.to_csv(config.output_dir / f'bridge_subject_fusion_weights_{timestamp}.csv', index=False)\n",
    "\n",
    "# Save XAI arrays\n",
    "np.savez(\n",
    "    config.output_dir / f'bridge_xai_arrays_{timestamp}.npz',\n",
    "    gradient_saliency_eeg=mean_eeg_sal,\n",
    "    gradient_saliency_fmri=mean_fmri_sal,\n",
    "    integrated_gradients_eeg=mean_eeg_ig,\n",
    "    integrated_gradients_fmri=mean_fmri_ig,\n",
    "    per_subject_fusion_weights=fusion_weights_arr,\n",
    ")\n",
    "\n",
    "logger.info(f'All results saved to {config.output_dir}')\n",
    "logger.info(f'Figures saved to {fig_dir}')\n",
    "logger.info(f'Checkpoints saved to {config.checkpoint_dir}')\n",
    "\n",
    "print(f'\\nOutput directory: {config.output_dir}')\n",
    "print(f'Figures directory: {fig_dir}')\n",
    "print(f'Checkpoints directory: {config.checkpoint_dir}')\n",
    "print(f'Timestamp: {timestamp}')\n",
    "print('\\nBridge fusion pipeline complete.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
